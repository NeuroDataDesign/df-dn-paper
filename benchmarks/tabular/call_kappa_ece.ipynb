{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af46221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "#%run create_params.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a287387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Dataset:  0\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 2.37175 |  0:00:00s\n",
      "epoch 1  | loss: 1.46474 |  0:00:00s\n",
      "epoch 2  | loss: 1.45166 |  0:00:00s\n",
      "epoch 3  | loss: 0.90884 |  0:00:00s\n",
      "epoch 4  | loss: 0.86081 |  0:00:00s\n",
      "epoch 5  | loss: 1.6017  |  0:00:00s\n",
      "epoch 6  | loss: 1.02068 |  0:00:00s\n",
      "epoch 7  | loss: 1.4734  |  0:00:00s\n",
      "epoch 8  | loss: 0.97165 |  0:00:00s\n",
      "epoch 9  | loss: 0.72757 |  0:00:00s\n",
      "epoch 10 | loss: 0.95151 |  0:00:00s\n",
      "epoch 11 | loss: 1.01741 |  0:00:00s\n",
      "epoch 12 | loss: 1.08218 |  0:00:01s\n",
      "epoch 13 | loss: 0.68702 |  0:00:01s\n",
      "epoch 14 | loss: 0.8023  |  0:00:01s\n",
      "epoch 15 | loss: 0.87479 |  0:00:01s\n",
      "epoch 16 | loss: 0.86579 |  0:00:01s\n",
      "epoch 17 | loss: 0.9773  |  0:00:01s\n",
      "epoch 18 | loss: 0.50087 |  0:00:01s\n",
      "epoch 19 | loss: 0.34849 |  0:00:01s\n",
      "epoch 20 | loss: 0.46568 |  0:00:01s\n",
      "epoch 21 | loss: 0.50124 |  0:00:01s\n",
      "epoch 22 | loss: 0.53685 |  0:00:01s\n",
      "epoch 23 | loss: 0.50596 |  0:00:01s\n",
      "epoch 24 | loss: 0.25919 |  0:00:01s\n",
      "epoch 25 | loss: 0.3108  |  0:00:01s\n",
      "epoch 26 | loss: 0.29055 |  0:00:01s\n",
      "epoch 27 | loss: 0.40127 |  0:00:02s\n",
      "epoch 28 | loss: 0.20617 |  0:00:02s\n",
      "epoch 29 | loss: 0.1275  |  0:00:02s\n",
      "epoch 30 | loss: 0.11145 |  0:00:02s\n",
      "epoch 31 | loss: 0.094   |  0:00:02s\n",
      "epoch 32 | loss: 0.09356 |  0:00:02s\n",
      "epoch 33 | loss: 0.10134 |  0:00:02s\n",
      "epoch 34 | loss: 0.09495 |  0:00:02s\n",
      "epoch 35 | loss: 0.10688 |  0:00:02s\n",
      "epoch 36 | loss: 0.09839 |  0:00:02s\n",
      "epoch 37 | loss: 0.07411 |  0:00:02s\n",
      "epoch 38 | loss: 0.07189 |  0:00:02s\n",
      "epoch 39 | loss: 0.08532 |  0:00:02s\n",
      "epoch 40 | loss: 0.07052 |  0:00:02s\n",
      "epoch 41 | loss: 0.06391 |  0:00:02s\n",
      "epoch 42 | loss: 0.07431 |  0:00:03s\n",
      "epoch 43 | loss: 0.08112 |  0:00:03s\n",
      "epoch 44 | loss: 0.06705 |  0:00:03s\n",
      "epoch 45 | loss: 0.07515 |  0:00:03s\n",
      "epoch 46 | loss: 0.06727 |  0:00:03s\n",
      "epoch 47 | loss: 0.06659 |  0:00:03s\n",
      "epoch 48 | loss: 0.06391 |  0:00:03s\n",
      "epoch 49 | loss: 0.06372 |  0:00:03s\n",
      "epoch 50 | loss: 0.06098 |  0:00:03s\n",
      "epoch 51 | loss: 0.05895 |  0:00:03s\n",
      "epoch 52 | loss: 0.05617 |  0:00:03s\n",
      "epoch 53 | loss: 0.0552  |  0:00:03s\n",
      "epoch 54 | loss: 0.057   |  0:00:03s\n",
      "epoch 55 | loss: 0.05361 |  0:00:03s\n",
      "epoch 56 | loss: 0.04948 |  0:00:03s\n",
      "epoch 57 | loss: 0.04502 |  0:00:03s\n",
      "epoch 58 | loss: 0.04381 |  0:00:04s\n",
      "epoch 59 | loss: 0.04212 |  0:00:04s\n",
      "epoch 60 | loss: 0.04612 |  0:00:04s\n",
      "epoch 61 | loss: 0.04158 |  0:00:04s\n",
      "epoch 62 | loss: 0.03907 |  0:00:04s\n",
      "epoch 63 | loss: 0.03803 |  0:00:04s\n",
      "epoch 64 | loss: 0.03643 |  0:00:04s\n",
      "epoch 65 | loss: 0.03567 |  0:00:04s\n",
      "epoch 66 | loss: 0.03412 |  0:00:04s\n",
      "epoch 67 | loss: 0.03275 |  0:00:04s\n",
      "epoch 68 | loss: 0.03127 |  0:00:04s\n",
      "epoch 69 | loss: 0.02966 |  0:00:04s\n",
      "epoch 70 | loss: 0.02853 |  0:00:04s\n",
      "epoch 71 | loss: 0.02758 |  0:00:04s\n",
      "epoch 72 | loss: 0.02625 |  0:00:04s\n",
      "epoch 73 | loss: 0.02539 |  0:00:04s\n",
      "epoch 74 | loss: 0.0243  |  0:00:04s\n",
      "epoch 75 | loss: 0.02327 |  0:00:05s\n",
      "epoch 76 | loss: 0.022   |  0:00:05s\n",
      "epoch 77 | loss: 0.02096 |  0:00:05s\n",
      "epoch 78 | loss: 0.01982 |  0:00:05s\n",
      "epoch 79 | loss: 0.01907 |  0:00:05s\n",
      "epoch 80 | loss: 0.01851 |  0:00:05s\n",
      "epoch 81 | loss: 0.01816 |  0:00:05s\n",
      "epoch 82 | loss: 0.01784 |  0:00:05s\n",
      "epoch 83 | loss: 0.01757 |  0:00:05s\n",
      "epoch 84 | loss: 0.01714 |  0:00:05s\n",
      "epoch 85 | loss: 0.01663 |  0:00:05s\n",
      "epoch 86 | loss: 0.0161  |  0:00:05s\n",
      "epoch 87 | loss: 0.01536 |  0:00:05s\n",
      "epoch 88 | loss: 0.01484 |  0:00:05s\n",
      "epoch 89 | loss: 0.01442 |  0:00:05s\n",
      "epoch 90 | loss: 0.01456 |  0:00:05s\n",
      "epoch 91 | loss: 0.01402 |  0:00:06s\n",
      "epoch 92 | loss: 0.01357 |  0:00:06s\n",
      "epoch 93 | loss: 0.01336 |  0:00:06s\n",
      "epoch 94 | loss: 0.01308 |  0:00:06s\n",
      "epoch 95 | loss: 0.01285 |  0:00:06s\n",
      "epoch 96 | loss: 0.01265 |  0:00:06s\n",
      "epoch 97 | loss: 0.01233 |  0:00:06s\n",
      "epoch 98 | loss: 0.01199 |  0:00:06s\n",
      "epoch 99 | loss: 0.0117  |  0:00:06s\n",
      "[14:33:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.09899 |  0:00:00s\n",
      "epoch 1  | loss: 1.27916 |  0:00:00s\n",
      "epoch 2  | loss: 1.10794 |  0:00:00s\n",
      "epoch 3  | loss: 0.76895 |  0:00:00s\n",
      "epoch 4  | loss: 0.77077 |  0:00:00s\n",
      "epoch 5  | loss: 0.91055 |  0:00:00s\n",
      "epoch 6  | loss: 0.60117 |  0:00:00s\n",
      "epoch 7  | loss: 0.81634 |  0:00:00s\n",
      "epoch 8  | loss: 0.4935  |  0:00:00s\n",
      "epoch 9  | loss: 0.59267 |  0:00:00s\n",
      "epoch 10 | loss: 0.33696 |  0:00:00s\n",
      "epoch 11 | loss: 0.60706 |  0:00:00s\n",
      "epoch 12 | loss: 0.30982 |  0:00:00s\n",
      "epoch 13 | loss: 0.3875  |  0:00:00s\n",
      "epoch 14 | loss: 0.3735  |  0:00:00s\n",
      "epoch 15 | loss: 0.39555 |  0:00:00s\n",
      "epoch 16 | loss: 0.27403 |  0:00:00s\n",
      "epoch 17 | loss: 0.39336 |  0:00:00s\n",
      "epoch 18 | loss: 0.4018  |  0:00:00s\n",
      "epoch 19 | loss: 0.26368 |  0:00:00s\n",
      "epoch 20 | loss: 0.3045  |  0:00:00s\n",
      "epoch 21 | loss: 0.38582 |  0:00:00s\n",
      "epoch 22 | loss: 0.22374 |  0:00:00s\n",
      "epoch 23 | loss: 0.30924 |  0:00:01s\n",
      "epoch 24 | loss: 0.45667 |  0:00:01s\n",
      "epoch 25 | loss: 0.46833 |  0:00:01s\n",
      "epoch 26 | loss: 0.26144 |  0:00:01s\n",
      "epoch 27 | loss: 0.20326 |  0:00:01s\n",
      "epoch 28 | loss: 0.21748 |  0:00:01s\n",
      "epoch 29 | loss: 0.09763 |  0:00:01s\n",
      "epoch 30 | loss: 0.08718 |  0:00:01s\n",
      "epoch 31 | loss: 0.07692 |  0:00:01s\n",
      "epoch 32 | loss: 0.03872 |  0:00:01s\n",
      "epoch 33 | loss: 0.02465 |  0:00:01s\n",
      "epoch 34 | loss: 0.01749 |  0:00:01s\n",
      "epoch 35 | loss: 0.02101 |  0:00:01s\n",
      "epoch 36 | loss: 0.01155 |  0:00:01s\n",
      "epoch 37 | loss: 0.01028 |  0:00:01s\n",
      "epoch 38 | loss: 0.00994 |  0:00:01s\n",
      "epoch 39 | loss: 0.00941 |  0:00:01s\n",
      "epoch 40 | loss: 0.0085  |  0:00:01s\n",
      "epoch 41 | loss: 0.00793 |  0:00:01s\n",
      "epoch 42 | loss: 0.0076  |  0:00:01s\n",
      "epoch 43 | loss: 0.00737 |  0:00:01s\n",
      "epoch 44 | loss: 0.00721 |  0:00:01s\n",
      "epoch 45 | loss: 0.00709 |  0:00:02s\n",
      "epoch 46 | loss: 0.00699 |  0:00:02s\n",
      "epoch 47 | loss: 0.00688 |  0:00:02s\n",
      "epoch 48 | loss: 0.00679 |  0:00:02s\n",
      "epoch 49 | loss: 0.00671 |  0:00:02s\n",
      "epoch 50 | loss: 0.00664 |  0:00:02s\n",
      "epoch 51 | loss: 0.00658 |  0:00:02s\n",
      "epoch 52 | loss: 0.00652 |  0:00:02s\n",
      "epoch 53 | loss: 0.00645 |  0:00:02s\n",
      "epoch 54 | loss: 0.00636 |  0:00:02s\n",
      "epoch 55 | loss: 0.00629 |  0:00:02s\n",
      "epoch 56 | loss: 0.00621 |  0:00:02s\n",
      "epoch 57 | loss: 0.00615 |  0:00:02s\n",
      "epoch 58 | loss: 0.0061  |  0:00:02s\n",
      "epoch 59 | loss: 0.00604 |  0:00:02s\n",
      "epoch 60 | loss: 0.00601 |  0:00:02s\n",
      "epoch 61 | loss: 0.00596 |  0:00:02s\n",
      "epoch 62 | loss: 0.0059  |  0:00:02s\n",
      "epoch 63 | loss: 0.00584 |  0:00:02s\n",
      "epoch 64 | loss: 0.00577 |  0:00:02s\n",
      "epoch 65 | loss: 0.0057  |  0:00:02s\n",
      "epoch 66 | loss: 0.00564 |  0:00:02s\n",
      "epoch 67 | loss: 0.00559 |  0:00:02s\n",
      "epoch 68 | loss: 0.00556 |  0:00:03s\n",
      "epoch 69 | loss: 0.00553 |  0:00:03s\n",
      "epoch 70 | loss: 0.00549 |  0:00:03s\n",
      "epoch 71 | loss: 0.00546 |  0:00:03s\n",
      "epoch 72 | loss: 0.00542 |  0:00:03s\n",
      "epoch 73 | loss: 0.00538 |  0:00:03s\n",
      "epoch 74 | loss: 0.00534 |  0:00:03s\n",
      "epoch 75 | loss: 0.00529 |  0:00:03s\n",
      "epoch 76 | loss: 0.00524 |  0:00:03s\n",
      "epoch 77 | loss: 0.00519 |  0:00:03s\n",
      "epoch 78 | loss: 0.00514 |  0:00:03s\n",
      "epoch 79 | loss: 0.00508 |  0:00:03s\n",
      "epoch 80 | loss: 0.00501 |  0:00:03s\n",
      "epoch 81 | loss: 0.00496 |  0:00:03s\n",
      "epoch 82 | loss: 0.00491 |  0:00:03s\n",
      "epoch 83 | loss: 0.00487 |  0:00:03s\n",
      "epoch 84 | loss: 0.00484 |  0:00:03s\n",
      "epoch 85 | loss: 0.00481 |  0:00:03s\n",
      "epoch 86 | loss: 0.00478 |  0:00:03s\n",
      "epoch 87 | loss: 0.00475 |  0:00:03s\n",
      "epoch 88 | loss: 0.00472 |  0:00:03s\n",
      "epoch 89 | loss: 0.0047  |  0:00:03s\n",
      "epoch 90 | loss: 0.00467 |  0:00:03s\n",
      "epoch 91 | loss: 0.00463 |  0:00:04s\n",
      "epoch 92 | loss: 0.00459 |  0:00:04s\n",
      "epoch 93 | loss: 0.00455 |  0:00:04s\n",
      "epoch 94 | loss: 0.00451 |  0:00:04s\n",
      "epoch 95 | loss: 0.00448 |  0:00:04s\n",
      "epoch 96 | loss: 0.00445 |  0:00:04s\n",
      "epoch 97 | loss: 0.00443 |  0:00:04s\n",
      "epoch 98 | loss: 0.0044  |  0:00:04s\n",
      "epoch 99 | loss: 0.00437 |  0:00:04s\n",
      "[14:33:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.96033 |  0:00:00s\n",
      "epoch 1  | loss: 1.50268 |  0:00:00s\n",
      "epoch 2  | loss: 1.08284 |  0:00:00s\n",
      "epoch 3  | loss: 1.29756 |  0:00:00s\n",
      "epoch 4  | loss: 1.19835 |  0:00:00s\n",
      "epoch 5  | loss: 1.35281 |  0:00:00s\n",
      "epoch 6  | loss: 1.29208 |  0:00:00s\n",
      "epoch 7  | loss: 1.551   |  0:00:00s\n",
      "epoch 8  | loss: 1.43929 |  0:00:00s\n",
      "epoch 9  | loss: 1.28527 |  0:00:00s\n",
      "epoch 10 | loss: 1.14703 |  0:00:00s\n",
      "epoch 11 | loss: 1.21114 |  0:00:00s\n",
      "epoch 12 | loss: 1.06401 |  0:00:00s\n",
      "epoch 13 | loss: 1.03252 |  0:00:00s\n",
      "epoch 14 | loss: 0.91138 |  0:00:00s\n",
      "epoch 15 | loss: 0.8598  |  0:00:00s\n",
      "epoch 16 | loss: 0.69804 |  0:00:00s\n",
      "epoch 17 | loss: 0.69556 |  0:00:01s\n",
      "epoch 18 | loss: 0.94139 |  0:00:01s\n",
      "epoch 19 | loss: 0.65413 |  0:00:01s\n",
      "epoch 20 | loss: 0.4347  |  0:00:01s\n",
      "epoch 21 | loss: 0.53832 |  0:00:01s\n",
      "epoch 22 | loss: 0.5373  |  0:00:01s\n",
      "epoch 23 | loss: 0.56489 |  0:00:01s\n",
      "epoch 24 | loss: 0.59678 |  0:00:01s\n",
      "epoch 25 | loss: 0.54236 |  0:00:01s\n",
      "epoch 26 | loss: 0.57783 |  0:00:01s\n",
      "epoch 27 | loss: 0.47262 |  0:00:01s\n",
      "epoch 28 | loss: 0.4886  |  0:00:01s\n",
      "epoch 29 | loss: 0.43265 |  0:00:01s\n",
      "epoch 30 | loss: 0.29141 |  0:00:01s\n",
      "epoch 31 | loss: 0.53672 |  0:00:01s\n",
      "epoch 32 | loss: 0.49293 |  0:00:01s\n",
      "epoch 33 | loss: 0.54947 |  0:00:01s\n",
      "epoch 34 | loss: 0.50948 |  0:00:01s\n",
      "epoch 35 | loss: 0.44007 |  0:00:02s\n",
      "epoch 36 | loss: 0.3997  |  0:00:02s\n",
      "epoch 37 | loss: 0.35946 |  0:00:02s\n",
      "epoch 38 | loss: 0.46973 |  0:00:02s\n",
      "epoch 39 | loss: 0.38426 |  0:00:02s\n",
      "epoch 40 | loss: 0.4685  |  0:00:02s\n",
      "epoch 41 | loss: 0.29876 |  0:00:02s\n",
      "epoch 42 | loss: 0.29647 |  0:00:02s\n",
      "epoch 43 | loss: 0.29803 |  0:00:02s\n",
      "epoch 44 | loss: 0.25405 |  0:00:02s\n",
      "epoch 45 | loss: 0.16442 |  0:00:02s\n",
      "epoch 46 | loss: 0.23587 |  0:00:02s\n",
      "epoch 47 | loss: 0.44128 |  0:00:02s\n",
      "epoch 48 | loss: 0.36009 |  0:00:02s\n",
      "epoch 49 | loss: 0.39642 |  0:00:02s\n",
      "epoch 50 | loss: 0.29116 |  0:00:02s\n",
      "epoch 51 | loss: 0.1732  |  0:00:02s\n",
      "epoch 52 | loss: 0.20782 |  0:00:02s\n",
      "epoch 53 | loss: 0.21248 |  0:00:03s\n",
      "epoch 54 | loss: 0.25514 |  0:00:03s\n",
      "epoch 55 | loss: 0.30954 |  0:00:03s\n",
      "epoch 56 | loss: 0.30436 |  0:00:03s\n",
      "epoch 57 | loss: 0.26692 |  0:00:03s\n",
      "epoch 58 | loss: 0.18814 |  0:00:03s\n",
      "epoch 59 | loss: 0.17631 |  0:00:03s\n",
      "epoch 60 | loss: 0.13156 |  0:00:03s\n",
      "epoch 61 | loss: 0.21046 |  0:00:03s\n",
      "epoch 62 | loss: 0.13762 |  0:00:03s\n",
      "epoch 63 | loss: 0.08275 |  0:00:03s\n",
      "epoch 64 | loss: 0.05189 |  0:00:03s\n",
      "epoch 65 | loss: 0.03326 |  0:00:03s\n",
      "epoch 66 | loss: 0.02226 |  0:00:03s\n",
      "epoch 67 | loss: 0.01541 |  0:00:03s\n",
      "epoch 68 | loss: 0.00919 |  0:00:03s\n",
      "epoch 69 | loss: 0.01938 |  0:00:03s\n",
      "epoch 70 | loss: 0.01329 |  0:00:03s\n",
      "epoch 71 | loss: 0.01059 |  0:00:04s\n",
      "epoch 72 | loss: 0.00947 |  0:00:04s\n",
      "epoch 73 | loss: 0.01061 |  0:00:04s\n",
      "epoch 74 | loss: 0.00168 |  0:00:04s\n",
      "epoch 75 | loss: 0.00179 |  0:00:04s\n",
      "epoch 76 | loss: 0.08495 |  0:00:04s\n",
      "epoch 77 | loss: 0.00671 |  0:00:04s\n",
      "epoch 78 | loss: 0.02866 |  0:00:04s\n",
      "epoch 79 | loss: 0.24584 |  0:00:04s\n",
      "epoch 80 | loss: 0.04689 |  0:00:04s\n",
      "epoch 81 | loss: 0.01855 |  0:00:04s\n",
      "epoch 82 | loss: 0.01097 |  0:00:04s\n",
      "epoch 83 | loss: 0.13679 |  0:00:04s\n",
      "epoch 84 | loss: 0.45581 |  0:00:04s\n",
      "epoch 85 | loss: 0.00575 |  0:00:04s\n",
      "epoch 86 | loss: 0.03475 |  0:00:04s\n",
      "epoch 87 | loss: 0.00631 |  0:00:04s\n",
      "epoch 88 | loss: 0.00254 |  0:00:04s\n",
      "epoch 89 | loss: 0.00203 |  0:00:05s\n",
      "epoch 90 | loss: 0.00212 |  0:00:05s\n",
      "epoch 91 | loss: 0.0014  |  0:00:05s\n",
      "epoch 92 | loss: 0.00122 |  0:00:05s\n",
      "epoch 93 | loss: 0.0009  |  0:00:05s\n",
      "epoch 94 | loss: 0.0008  |  0:00:05s\n",
      "epoch 95 | loss: 0.00053 |  0:00:05s\n",
      "epoch 96 | loss: 0.00108 |  0:00:05s\n",
      "epoch 97 | loss: 0.00098 |  0:00:06s\n",
      "epoch 98 | loss: 0.0008  |  0:00:06s\n",
      "epoch 99 | loss: 0.17376 |  0:00:06s\n",
      "[14:33:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 3.8337  |  0:00:00s\n",
      "epoch 1  | loss: 3.47912 |  0:00:00s\n",
      "epoch 2  | loss: 1.84334 |  0:00:00s\n",
      "epoch 3  | loss: 1.52741 |  0:00:00s\n",
      "epoch 4  | loss: 1.04863 |  0:00:00s\n",
      "epoch 5  | loss: 0.78537 |  0:00:00s\n",
      "epoch 6  | loss: 1.27258 |  0:00:01s\n",
      "epoch 7  | loss: 1.03999 |  0:00:01s\n",
      "epoch 8  | loss: 1.06168 |  0:00:01s\n",
      "epoch 9  | loss: 1.12504 |  0:00:01s\n",
      "epoch 10 | loss: 0.86983 |  0:00:01s\n",
      "epoch 11 | loss: 1.02653 |  0:00:01s\n",
      "epoch 12 | loss: 0.6861  |  0:00:01s\n",
      "epoch 13 | loss: 0.63442 |  0:00:01s\n",
      "epoch 14 | loss: 0.59223 |  0:00:02s\n",
      "epoch 15 | loss: 0.58344 |  0:00:02s\n",
      "epoch 16 | loss: 0.52946 |  0:00:02s\n",
      "epoch 17 | loss: 0.62911 |  0:00:02s\n",
      "epoch 18 | loss: 0.60015 |  0:00:02s\n",
      "epoch 19 | loss: 0.47164 |  0:00:02s\n",
      "epoch 20 | loss: 0.6215  |  0:00:02s\n",
      "epoch 21 | loss: 0.58903 |  0:00:02s\n",
      "epoch 22 | loss: 0.48527 |  0:00:03s\n",
      "epoch 23 | loss: 0.504   |  0:00:03s\n",
      "epoch 24 | loss: 0.38239 |  0:00:03s\n",
      "epoch 25 | loss: 0.46816 |  0:00:03s\n",
      "epoch 26 | loss: 0.30272 |  0:00:03s\n",
      "epoch 27 | loss: 0.48706 |  0:00:03s\n",
      "epoch 28 | loss: 0.34122 |  0:00:03s\n",
      "epoch 29 | loss: 0.42135 |  0:00:03s\n",
      "epoch 30 | loss: 0.32885 |  0:00:04s\n",
      "epoch 31 | loss: 0.42633 |  0:00:04s\n",
      "epoch 32 | loss: 0.33384 |  0:00:04s\n",
      "epoch 33 | loss: 0.2371  |  0:00:04s\n",
      "epoch 34 | loss: 0.28809 |  0:00:04s\n",
      "epoch 35 | loss: 0.43646 |  0:00:04s\n",
      "epoch 36 | loss: 0.33798 |  0:00:05s\n",
      "epoch 37 | loss: 0.6031  |  0:00:05s\n",
      "epoch 38 | loss: 0.66085 |  0:00:05s\n",
      "epoch 39 | loss: 0.63919 |  0:00:05s\n",
      "epoch 40 | loss: 0.32077 |  0:00:05s\n",
      "epoch 41 | loss: 0.58567 |  0:00:05s\n",
      "epoch 42 | loss: 0.61813 |  0:00:05s\n",
      "epoch 43 | loss: 0.60514 |  0:00:05s\n",
      "epoch 44 | loss: 0.4778  |  0:00:06s\n",
      "epoch 45 | loss: 0.58609 |  0:00:06s\n",
      "epoch 46 | loss: 0.30248 |  0:00:06s\n",
      "epoch 47 | loss: 0.39296 |  0:00:06s\n",
      "epoch 48 | loss: 0.40053 |  0:00:06s\n",
      "epoch 49 | loss: 0.34738 |  0:00:06s\n",
      "epoch 50 | loss: 0.36425 |  0:00:06s\n",
      "epoch 51 | loss: 0.25212 |  0:00:06s\n",
      "epoch 52 | loss: 0.25075 |  0:00:06s\n",
      "epoch 53 | loss: 0.20058 |  0:00:07s\n",
      "epoch 54 | loss: 0.27319 |  0:00:07s\n",
      "epoch 55 | loss: 0.21845 |  0:00:07s\n",
      "epoch 56 | loss: 0.31013 |  0:00:07s\n",
      "epoch 57 | loss: 0.27919 |  0:00:07s\n",
      "epoch 58 | loss: 0.27406 |  0:00:07s\n",
      "epoch 59 | loss: 0.28305 |  0:00:07s\n",
      "epoch 60 | loss: 0.5562  |  0:00:07s\n",
      "epoch 61 | loss: 0.41233 |  0:00:08s\n",
      "epoch 62 | loss: 0.36139 |  0:00:08s\n",
      "epoch 63 | loss: 0.24816 |  0:00:08s\n",
      "epoch 64 | loss: 0.43388 |  0:00:08s\n",
      "epoch 65 | loss: 0.31015 |  0:00:08s\n",
      "epoch 66 | loss: 0.25083 |  0:00:08s\n",
      "epoch 67 | loss: 0.51659 |  0:00:08s\n",
      "epoch 68 | loss: 0.2105  |  0:00:08s\n",
      "epoch 69 | loss: 0.28583 |  0:00:08s\n",
      "epoch 70 | loss: 0.29799 |  0:00:08s\n",
      "epoch 71 | loss: 0.34441 |  0:00:09s\n",
      "epoch 72 | loss: 0.28213 |  0:00:09s\n",
      "epoch 73 | loss: 0.17551 |  0:00:09s\n",
      "epoch 74 | loss: 0.22092 |  0:00:09s\n",
      "epoch 75 | loss: 0.20044 |  0:00:09s\n",
      "epoch 76 | loss: 0.26594 |  0:00:09s\n",
      "epoch 77 | loss: 0.13435 |  0:00:09s\n",
      "epoch 78 | loss: 0.15231 |  0:00:09s\n",
      "epoch 79 | loss: 0.39696 |  0:00:10s\n",
      "epoch 80 | loss: 0.25667 |  0:00:10s\n",
      "epoch 81 | loss: 0.15603 |  0:00:10s\n",
      "epoch 82 | loss: 0.20812 |  0:00:10s\n",
      "epoch 83 | loss: 0.17979 |  0:00:10s\n",
      "epoch 84 | loss: 0.23453 |  0:00:10s\n",
      "epoch 85 | loss: 0.13951 |  0:00:10s\n",
      "epoch 86 | loss: 0.11367 |  0:00:10s\n",
      "epoch 87 | loss: 0.10254 |  0:00:11s\n",
      "epoch 88 | loss: 0.1198  |  0:00:11s\n",
      "epoch 89 | loss: 0.1087  |  0:00:11s\n",
      "epoch 90 | loss: 0.08402 |  0:00:11s\n",
      "epoch 91 | loss: 0.10844 |  0:00:11s\n",
      "epoch 92 | loss: 0.04272 |  0:00:11s\n",
      "epoch 93 | loss: 0.03576 |  0:00:11s\n",
      "epoch 94 | loss: 0.18576 |  0:00:11s\n",
      "epoch 95 | loss: 0.11017 |  0:00:11s\n",
      "epoch 96 | loss: 0.05734 |  0:00:12s\n",
      "epoch 97 | loss: 0.03621 |  0:00:12s\n",
      "epoch 98 | loss: 0.11639 |  0:00:12s\n",
      "epoch 99 | loss: 0.07409 |  0:00:12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:34:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.70325 |  0:00:00s\n",
      "epoch 1  | loss: 1.25264 |  0:00:00s\n",
      "epoch 2  | loss: 1.08757 |  0:00:00s\n",
      "epoch 3  | loss: 0.82023 |  0:00:00s\n",
      "epoch 4  | loss: 0.88603 |  0:00:00s\n",
      "epoch 5  | loss: 0.97065 |  0:00:00s\n",
      "epoch 6  | loss: 0.78702 |  0:00:00s\n",
      "epoch 7  | loss: 0.94689 |  0:00:01s\n",
      "epoch 8  | loss: 0.74655 |  0:00:01s\n",
      "epoch 9  | loss: 0.8004  |  0:00:01s\n",
      "epoch 10 | loss: 0.82628 |  0:00:01s\n",
      "epoch 11 | loss: 0.69991 |  0:00:01s\n",
      "epoch 12 | loss: 0.78979 |  0:00:01s\n",
      "epoch 13 | loss: 0.80005 |  0:00:01s\n",
      "epoch 14 | loss: 0.75054 |  0:00:01s\n",
      "epoch 15 | loss: 0.71289 |  0:00:01s\n",
      "epoch 16 | loss: 0.6522  |  0:00:01s\n",
      "epoch 17 | loss: 0.6683  |  0:00:02s\n",
      "epoch 18 | loss: 0.62635 |  0:00:02s\n",
      "epoch 19 | loss: 0.6617  |  0:00:02s\n",
      "epoch 20 | loss: 0.67382 |  0:00:02s\n",
      "epoch 21 | loss: 0.57056 |  0:00:02s\n",
      "epoch 22 | loss: 0.57764 |  0:00:02s\n",
      "epoch 23 | loss: 0.59811 |  0:00:02s\n",
      "epoch 24 | loss: 0.48424 |  0:00:02s\n",
      "epoch 25 | loss: 0.62025 |  0:00:02s\n",
      "epoch 26 | loss: 0.47149 |  0:00:02s\n",
      "epoch 27 | loss: 0.54157 |  0:00:02s\n",
      "epoch 28 | loss: 0.46629 |  0:00:03s\n",
      "epoch 29 | loss: 0.43119 |  0:00:03s\n",
      "epoch 30 | loss: 0.45954 |  0:00:03s\n",
      "epoch 31 | loss: 0.49869 |  0:00:03s\n",
      "epoch 32 | loss: 0.48538 |  0:00:03s\n",
      "epoch 33 | loss: 0.44104 |  0:00:03s\n",
      "epoch 34 | loss: 0.48025 |  0:00:03s\n",
      "epoch 35 | loss: 0.42887 |  0:00:03s\n",
      "epoch 36 | loss: 0.40525 |  0:00:03s\n",
      "epoch 37 | loss: 0.40387 |  0:00:04s\n",
      "epoch 38 | loss: 0.33303 |  0:00:04s\n",
      "epoch 39 | loss: 0.41868 |  0:00:04s\n",
      "epoch 40 | loss: 0.36571 |  0:00:04s\n",
      "epoch 41 | loss: 0.33329 |  0:00:04s\n",
      "epoch 42 | loss: 0.39674 |  0:00:04s\n",
      "epoch 43 | loss: 0.43133 |  0:00:04s\n",
      "epoch 44 | loss: 0.40204 |  0:00:04s\n",
      "epoch 45 | loss: 0.3512  |  0:00:04s\n",
      "epoch 46 | loss: 0.30452 |  0:00:05s\n",
      "epoch 47 | loss: 0.32647 |  0:00:05s\n",
      "epoch 48 | loss: 0.28957 |  0:00:05s\n",
      "epoch 49 | loss: 0.26495 |  0:00:05s\n",
      "epoch 50 | loss: 0.26617 |  0:00:05s\n",
      "epoch 51 | loss: 0.33902 |  0:00:05s\n",
      "epoch 52 | loss: 0.37658 |  0:00:05s\n",
      "epoch 53 | loss: 0.29086 |  0:00:05s\n",
      "epoch 54 | loss: 0.25291 |  0:00:05s\n",
      "epoch 55 | loss: 0.22999 |  0:00:06s\n",
      "epoch 56 | loss: 0.22186 |  0:00:06s\n",
      "epoch 57 | loss: 0.20987 |  0:00:06s\n",
      "epoch 58 | loss: 0.21186 |  0:00:06s\n",
      "epoch 59 | loss: 0.20585 |  0:00:06s\n",
      "epoch 60 | loss: 0.13843 |  0:00:06s\n",
      "epoch 61 | loss: 0.14618 |  0:00:06s\n",
      "epoch 62 | loss: 0.14361 |  0:00:06s\n",
      "epoch 63 | loss: 0.17379 |  0:00:06s\n",
      "epoch 64 | loss: 0.14098 |  0:00:06s\n",
      "epoch 65 | loss: 0.10095 |  0:00:07s\n",
      "epoch 66 | loss: 0.10587 |  0:00:07s\n",
      "epoch 67 | loss: 0.11084 |  0:00:07s\n",
      "epoch 68 | loss: 0.09324 |  0:00:07s\n",
      "epoch 69 | loss: 0.13208 |  0:00:07s\n",
      "epoch 70 | loss: 0.07583 |  0:00:07s\n",
      "epoch 71 | loss: 0.09018 |  0:00:07s\n",
      "epoch 72 | loss: 0.09346 |  0:00:07s\n",
      "epoch 73 | loss: 0.08972 |  0:00:07s\n",
      "epoch 74 | loss: 0.04894 |  0:00:07s\n",
      "epoch 75 | loss: 0.02488 |  0:00:07s\n",
      "epoch 76 | loss: 0.04966 |  0:00:08s\n",
      "epoch 77 | loss: 0.03146 |  0:00:08s\n",
      "epoch 78 | loss: 0.02289 |  0:00:08s\n",
      "epoch 79 | loss: 0.02433 |  0:00:08s\n",
      "epoch 80 | loss: 0.13479 |  0:00:08s\n",
      "epoch 81 | loss: 0.22432 |  0:00:08s\n",
      "epoch 82 | loss: 0.12728 |  0:00:08s\n",
      "epoch 83 | loss: 0.09384 |  0:00:08s\n",
      "epoch 84 | loss: 0.05661 |  0:00:08s\n",
      "epoch 85 | loss: 0.03453 |  0:00:08s\n",
      "epoch 86 | loss: 0.02518 |  0:00:08s\n",
      "epoch 87 | loss: 0.05971 |  0:00:09s\n",
      "epoch 88 | loss: 0.02025 |  0:00:09s\n",
      "epoch 89 | loss: 0.05695 |  0:00:09s\n",
      "epoch 90 | loss: 0.06214 |  0:00:09s\n",
      "epoch 91 | loss: 0.1206  |  0:00:09s\n",
      "epoch 92 | loss: 0.0697  |  0:00:09s\n",
      "epoch 93 | loss: 0.0331  |  0:00:09s\n",
      "epoch 94 | loss: 0.04364 |  0:00:09s\n",
      "epoch 95 | loss: 0.05317 |  0:00:10s\n",
      "epoch 96 | loss: 0.03603 |  0:00:10s\n",
      "epoch 97 | loss: 0.16204 |  0:00:10s\n",
      "epoch 98 | loss: 0.07266 |  0:00:10s\n",
      "epoch 99 | loss: 0.034   |  0:00:10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:34:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.00373 |  0:00:00s\n",
      "epoch 1  | loss: 0.98285 |  0:00:00s\n",
      "epoch 2  | loss: 0.88016 |  0:00:00s\n",
      "epoch 3  | loss: 0.81731 |  0:00:00s\n",
      "epoch 4  | loss: 0.88326 |  0:00:00s\n",
      "epoch 5  | loss: 0.76181 |  0:00:00s\n",
      "epoch 6  | loss: 0.71542 |  0:00:00s\n",
      "epoch 7  | loss: 0.63783 |  0:00:00s\n",
      "epoch 8  | loss: 0.67842 |  0:00:00s\n",
      "epoch 9  | loss: 0.63587 |  0:00:00s\n",
      "epoch 10 | loss: 0.5769  |  0:00:00s\n",
      "epoch 11 | loss: 0.55214 |  0:00:00s\n",
      "epoch 12 | loss: 0.53201 |  0:00:00s\n",
      "epoch 13 | loss: 0.49119 |  0:00:00s\n",
      "epoch 14 | loss: 0.53824 |  0:00:00s\n",
      "epoch 15 | loss: 0.47421 |  0:00:00s\n",
      "epoch 16 | loss: 0.45083 |  0:00:00s\n",
      "epoch 17 | loss: 0.42523 |  0:00:00s\n",
      "epoch 18 | loss: 0.40808 |  0:00:00s\n",
      "epoch 19 | loss: 0.41714 |  0:00:00s\n",
      "epoch 20 | loss: 0.38787 |  0:00:00s\n",
      "epoch 21 | loss: 0.35781 |  0:00:00s\n",
      "epoch 22 | loss: 0.35476 |  0:00:01s\n",
      "epoch 23 | loss: 0.31335 |  0:00:01s\n",
      "epoch 24 | loss: 0.3266  |  0:00:01s\n",
      "epoch 25 | loss: 0.35812 |  0:00:01s\n",
      "epoch 26 | loss: 0.29771 |  0:00:01s\n",
      "epoch 27 | loss: 0.27513 |  0:00:01s\n",
      "epoch 28 | loss: 0.28571 |  0:00:01s\n",
      "epoch 29 | loss: 0.28727 |  0:00:01s\n",
      "epoch 30 | loss: 0.30435 |  0:00:01s\n",
      "epoch 31 | loss: 0.26526 |  0:00:01s\n",
      "epoch 32 | loss: 0.32064 |  0:00:01s\n",
      "epoch 33 | loss: 0.28113 |  0:00:01s\n",
      "epoch 34 | loss: 0.3118  |  0:00:01s\n",
      "epoch 35 | loss: 0.2611  |  0:00:01s\n",
      "epoch 36 | loss: 0.2934  |  0:00:01s\n",
      "epoch 37 | loss: 0.30867 |  0:00:01s\n",
      "epoch 38 | loss: 0.26607 |  0:00:01s\n",
      "epoch 39 | loss: 0.21863 |  0:00:01s\n",
      "epoch 40 | loss: 0.21161 |  0:00:01s\n",
      "epoch 41 | loss: 0.20169 |  0:00:01s\n",
      "epoch 42 | loss: 0.18354 |  0:00:01s\n",
      "epoch 43 | loss: 0.27362 |  0:00:01s\n",
      "epoch 44 | loss: 0.26361 |  0:00:02s\n",
      "epoch 45 | loss: 0.20793 |  0:00:02s\n",
      "epoch 46 | loss: 0.20501 |  0:00:02s\n",
      "epoch 47 | loss: 0.24869 |  0:00:02s\n",
      "epoch 48 | loss: 0.23246 |  0:00:02s\n",
      "epoch 49 | loss: 0.21155 |  0:00:02s\n",
      "epoch 50 | loss: 0.19697 |  0:00:02s\n",
      "epoch 51 | loss: 0.18676 |  0:00:02s\n",
      "epoch 52 | loss: 0.17386 |  0:00:02s\n",
      "epoch 53 | loss: 0.19325 |  0:00:02s\n",
      "epoch 54 | loss: 0.1932  |  0:00:02s\n",
      "epoch 55 | loss: 0.19011 |  0:00:02s\n",
      "epoch 56 | loss: 0.17859 |  0:00:02s\n",
      "epoch 57 | loss: 0.15157 |  0:00:02s\n",
      "epoch 58 | loss: 0.14381 |  0:00:02s\n",
      "epoch 59 | loss: 0.21299 |  0:00:02s\n",
      "epoch 60 | loss: 0.15798 |  0:00:02s\n",
      "epoch 61 | loss: 0.13081 |  0:00:02s\n",
      "epoch 62 | loss: 0.14769 |  0:00:02s\n",
      "epoch 63 | loss: 0.12839 |  0:00:02s\n",
      "epoch 64 | loss: 0.11098 |  0:00:02s\n",
      "epoch 65 | loss: 0.12194 |  0:00:02s\n",
      "epoch 66 | loss: 0.12229 |  0:00:02s\n",
      "epoch 67 | loss: 0.10637 |  0:00:03s\n",
      "epoch 68 | loss: 0.13367 |  0:00:03s\n",
      "epoch 69 | loss: 0.10429 |  0:00:03s\n",
      "epoch 70 | loss: 0.0976  |  0:00:03s\n",
      "epoch 71 | loss: 0.10414 |  0:00:03s\n",
      "epoch 72 | loss: 0.08589 |  0:00:03s\n",
      "epoch 73 | loss: 0.12134 |  0:00:03s\n",
      "epoch 74 | loss: 0.08928 |  0:00:03s\n",
      "epoch 75 | loss: 0.09656 |  0:00:03s\n",
      "epoch 76 | loss: 0.10213 |  0:00:03s\n",
      "epoch 77 | loss: 0.10623 |  0:00:03s\n",
      "epoch 78 | loss: 0.0899  |  0:00:03s\n",
      "epoch 79 | loss: 0.07929 |  0:00:03s\n",
      "epoch 80 | loss: 0.06858 |  0:00:03s\n",
      "epoch 81 | loss: 0.12954 |  0:00:03s\n",
      "epoch 82 | loss: 0.08121 |  0:00:03s\n",
      "epoch 83 | loss: 0.0716  |  0:00:03s\n",
      "epoch 84 | loss: 0.09053 |  0:00:03s\n",
      "epoch 85 | loss: 0.13881 |  0:00:03s\n",
      "epoch 86 | loss: 0.1078  |  0:00:03s\n",
      "epoch 87 | loss: 0.09376 |  0:00:03s\n",
      "epoch 88 | loss: 0.05844 |  0:00:04s\n",
      "epoch 89 | loss: 0.0547  |  0:00:04s\n",
      "epoch 90 | loss: 0.04085 |  0:00:04s\n",
      "epoch 91 | loss: 0.04702 |  0:00:04s\n",
      "epoch 92 | loss: 0.05549 |  0:00:04s\n",
      "epoch 93 | loss: 0.04634 |  0:00:04s\n",
      "epoch 94 | loss: 0.04252 |  0:00:04s\n",
      "epoch 95 | loss: 0.09165 |  0:00:04s\n",
      "epoch 96 | loss: 0.06457 |  0:00:04s\n",
      "epoch 97 | loss: 0.05266 |  0:00:04s\n",
      "epoch 98 | loss: 0.07812 |  0:00:04s\n",
      "epoch 99 | loss: 0.05545 |  0:00:04s\n",
      "[14:34:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.99937 |  0:00:00s\n",
      "epoch 1  | loss: 0.90945 |  0:00:00s\n",
      "epoch 2  | loss: 0.75641 |  0:00:00s\n",
      "epoch 3  | loss: 0.7016  |  0:00:00s\n",
      "epoch 4  | loss: 0.69314 |  0:00:00s\n",
      "epoch 5  | loss: 0.68586 |  0:00:00s\n",
      "epoch 6  | loss: 0.67249 |  0:00:00s\n",
      "epoch 7  | loss: 0.67692 |  0:00:00s\n",
      "epoch 8  | loss: 0.6602  |  0:00:00s\n",
      "epoch 9  | loss: 0.61932 |  0:00:00s\n",
      "epoch 10 | loss: 0.62992 |  0:00:00s\n",
      "epoch 11 | loss: 0.59722 |  0:00:01s\n",
      "epoch 12 | loss: 0.54761 |  0:00:01s\n",
      "epoch 13 | loss: 0.5966  |  0:00:01s\n",
      "epoch 14 | loss: 0.48866 |  0:00:01s\n",
      "epoch 15 | loss: 0.54141 |  0:00:01s\n",
      "epoch 16 | loss: 0.4657  |  0:00:01s\n",
      "epoch 17 | loss: 0.51833 |  0:00:01s\n",
      "epoch 18 | loss: 0.47709 |  0:00:01s\n",
      "epoch 19 | loss: 0.46908 |  0:00:01s\n",
      "epoch 20 | loss: 0.44703 |  0:00:01s\n",
      "epoch 21 | loss: 0.38792 |  0:00:01s\n",
      "epoch 22 | loss: 0.40099 |  0:00:01s\n",
      "epoch 23 | loss: 0.43001 |  0:00:02s\n",
      "epoch 24 | loss: 0.45829 |  0:00:02s\n",
      "epoch 25 | loss: 0.37737 |  0:00:02s\n",
      "epoch 26 | loss: 0.3979  |  0:00:02s\n",
      "epoch 27 | loss: 0.41701 |  0:00:02s\n",
      "epoch 28 | loss: 0.38192 |  0:00:02s\n",
      "epoch 29 | loss: 0.42275 |  0:00:02s\n",
      "epoch 30 | loss: 0.36126 |  0:00:02s\n",
      "epoch 31 | loss: 0.37274 |  0:00:02s\n",
      "epoch 32 | loss: 0.34491 |  0:00:02s\n",
      "epoch 33 | loss: 0.36772 |  0:00:03s\n",
      "epoch 34 | loss: 0.32195 |  0:00:03s\n",
      "epoch 35 | loss: 0.28593 |  0:00:03s\n",
      "epoch 36 | loss: 0.34066 |  0:00:03s\n",
      "epoch 37 | loss: 0.37229 |  0:00:03s\n",
      "epoch 38 | loss: 0.35607 |  0:00:03s\n",
      "epoch 39 | loss: 0.34383 |  0:00:03s\n",
      "epoch 40 | loss: 0.30052 |  0:00:03s\n",
      "epoch 41 | loss: 0.29164 |  0:00:03s\n",
      "epoch 42 | loss: 0.26335 |  0:00:03s\n",
      "epoch 43 | loss: 0.25213 |  0:00:03s\n",
      "epoch 44 | loss: 0.25285 |  0:00:04s\n",
      "epoch 45 | loss: 0.25084 |  0:00:04s\n",
      "epoch 46 | loss: 0.23286 |  0:00:04s\n",
      "epoch 47 | loss: 0.23821 |  0:00:04s\n",
      "epoch 48 | loss: 0.26187 |  0:00:04s\n",
      "epoch 49 | loss: 0.25328 |  0:00:04s\n",
      "epoch 50 | loss: 0.25939 |  0:00:04s\n",
      "epoch 51 | loss: 0.23459 |  0:00:04s\n",
      "epoch 52 | loss: 0.22992 |  0:00:04s\n",
      "epoch 53 | loss: 0.24324 |  0:00:04s\n",
      "epoch 54 | loss: 0.21486 |  0:00:04s\n",
      "epoch 55 | loss: 0.24377 |  0:00:05s\n",
      "epoch 56 | loss: 0.20516 |  0:00:05s\n",
      "epoch 57 | loss: 0.18664 |  0:00:05s\n",
      "epoch 58 | loss: 0.18035 |  0:00:05s\n",
      "epoch 59 | loss: 0.20044 |  0:00:05s\n",
      "epoch 60 | loss: 0.17312 |  0:00:05s\n",
      "epoch 61 | loss: 0.1801  |  0:00:05s\n",
      "epoch 62 | loss: 0.21113 |  0:00:05s\n",
      "epoch 63 | loss: 0.17774 |  0:00:05s\n",
      "epoch 64 | loss: 0.21456 |  0:00:05s\n",
      "epoch 65 | loss: 0.17818 |  0:00:05s\n",
      "epoch 66 | loss: 0.16641 |  0:00:05s\n",
      "epoch 67 | loss: 0.18327 |  0:00:06s\n",
      "epoch 68 | loss: 0.1796  |  0:00:06s\n",
      "epoch 69 | loss: 0.13913 |  0:00:06s\n",
      "epoch 70 | loss: 0.12614 |  0:00:06s\n",
      "epoch 71 | loss: 0.19151 |  0:00:06s\n",
      "epoch 72 | loss: 0.15209 |  0:00:06s\n",
      "epoch 73 | loss: 0.13324 |  0:00:06s\n",
      "epoch 74 | loss: 0.15952 |  0:00:06s\n",
      "epoch 75 | loss: 0.12355 |  0:00:06s\n",
      "epoch 76 | loss: 0.22188 |  0:00:06s\n",
      "epoch 77 | loss: 0.17966 |  0:00:06s\n",
      "epoch 78 | loss: 0.14091 |  0:00:07s\n",
      "epoch 79 | loss: 0.16405 |  0:00:07s\n",
      "epoch 80 | loss: 0.11309 |  0:00:07s\n",
      "epoch 81 | loss: 0.13066 |  0:00:07s\n",
      "epoch 82 | loss: 0.15443 |  0:00:07s\n",
      "epoch 83 | loss: 0.10874 |  0:00:07s\n",
      "epoch 84 | loss: 0.1579  |  0:00:07s\n",
      "epoch 85 | loss: 0.10172 |  0:00:07s\n",
      "epoch 86 | loss: 0.10675 |  0:00:07s\n",
      "epoch 87 | loss: 0.08721 |  0:00:07s\n",
      "epoch 88 | loss: 0.10923 |  0:00:07s\n",
      "epoch 89 | loss: 0.09739 |  0:00:07s\n",
      "epoch 90 | loss: 0.12094 |  0:00:08s\n",
      "epoch 91 | loss: 0.11332 |  0:00:08s\n",
      "epoch 92 | loss: 0.09753 |  0:00:08s\n",
      "epoch 93 | loss: 0.09457 |  0:00:08s\n",
      "epoch 94 | loss: 0.10069 |  0:00:08s\n",
      "epoch 95 | loss: 0.10252 |  0:00:08s\n",
      "epoch 96 | loss: 0.06926 |  0:00:08s\n",
      "epoch 97 | loss: 0.06131 |  0:00:08s\n",
      "epoch 98 | loss: 0.0567  |  0:00:08s\n",
      "epoch 99 | loss: 0.05781 |  0:00:08s\n",
      "[14:34:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.84024 |  0:00:00s\n",
      "epoch 1  | loss: 0.69435 |  0:00:00s\n",
      "epoch 2  | loss: 0.68492 |  0:00:00s\n",
      "epoch 3  | loss: 0.65718 |  0:00:00s\n",
      "epoch 4  | loss: 0.63858 |  0:00:00s\n",
      "epoch 5  | loss: 0.63143 |  0:00:00s\n",
      "epoch 6  | loss: 0.61764 |  0:00:00s\n",
      "epoch 7  | loss: 0.59993 |  0:00:00s\n",
      "epoch 8  | loss: 0.58261 |  0:00:00s\n",
      "epoch 9  | loss: 0.55777 |  0:00:00s\n",
      "epoch 10 | loss: 0.54863 |  0:00:00s\n",
      "epoch 11 | loss: 0.53419 |  0:00:00s\n",
      "epoch 12 | loss: 0.51024 |  0:00:00s\n",
      "epoch 13 | loss: 0.50221 |  0:00:00s\n",
      "epoch 14 | loss: 0.45094 |  0:00:01s\n",
      "epoch 15 | loss: 0.43925 |  0:00:01s\n",
      "epoch 16 | loss: 0.41772 |  0:00:01s\n",
      "epoch 17 | loss: 0.41244 |  0:00:01s\n",
      "epoch 18 | loss: 0.3997  |  0:00:01s\n",
      "epoch 19 | loss: 0.33797 |  0:00:01s\n",
      "epoch 20 | loss: 0.34652 |  0:00:01s\n",
      "epoch 21 | loss: 0.30844 |  0:00:01s\n",
      "epoch 22 | loss: 0.30946 |  0:00:01s\n",
      "epoch 23 | loss: 0.26453 |  0:00:01s\n",
      "epoch 24 | loss: 0.28897 |  0:00:01s\n",
      "epoch 25 | loss: 0.24441 |  0:00:01s\n",
      "epoch 26 | loss: 0.2232  |  0:00:01s\n",
      "epoch 27 | loss: 0.20623 |  0:00:01s\n",
      "epoch 28 | loss: 0.22637 |  0:00:02s\n",
      "epoch 29 | loss: 0.1843  |  0:00:02s\n",
      "epoch 30 | loss: 0.15407 |  0:00:02s\n",
      "epoch 31 | loss: 0.14387 |  0:00:02s\n",
      "epoch 32 | loss: 0.15725 |  0:00:02s\n",
      "epoch 33 | loss: 0.11736 |  0:00:02s\n",
      "epoch 34 | loss: 0.12029 |  0:00:02s\n",
      "epoch 35 | loss: 0.10073 |  0:00:02s\n",
      "epoch 36 | loss: 0.08433 |  0:00:02s\n",
      "epoch 37 | loss: 0.09603 |  0:00:02s\n",
      "epoch 38 | loss: 0.14445 |  0:00:02s\n",
      "epoch 39 | loss: 0.08547 |  0:00:02s\n",
      "epoch 40 | loss: 0.10502 |  0:00:02s\n",
      "epoch 41 | loss: 0.10941 |  0:00:02s\n",
      "epoch 42 | loss: 0.1258  |  0:00:03s\n",
      "epoch 43 | loss: 0.07634 |  0:00:03s\n",
      "epoch 44 | loss: 0.05577 |  0:00:03s\n",
      "epoch 45 | loss: 0.09633 |  0:00:03s\n",
      "epoch 46 | loss: 0.11396 |  0:00:03s\n",
      "epoch 47 | loss: 0.11996 |  0:00:03s\n",
      "epoch 48 | loss: 0.07432 |  0:00:03s\n",
      "epoch 49 | loss: 0.08152 |  0:00:03s\n",
      "epoch 50 | loss: 0.08127 |  0:00:03s\n",
      "epoch 51 | loss: 0.10342 |  0:00:03s\n",
      "epoch 52 | loss: 0.07854 |  0:00:03s\n",
      "epoch 53 | loss: 0.10836 |  0:00:03s\n",
      "epoch 54 | loss: 0.08167 |  0:00:03s\n",
      "epoch 55 | loss: 0.08475 |  0:00:03s\n",
      "epoch 56 | loss: 0.08823 |  0:00:04s\n",
      "epoch 57 | loss: 0.08745 |  0:00:04s\n",
      "epoch 58 | loss: 0.07696 |  0:00:04s\n",
      "epoch 59 | loss: 0.05648 |  0:00:04s\n",
      "epoch 60 | loss: 0.06112 |  0:00:04s\n",
      "epoch 61 | loss: 0.06521 |  0:00:04s\n",
      "epoch 62 | loss: 0.07373 |  0:00:04s\n",
      "epoch 63 | loss: 0.06355 |  0:00:04s\n",
      "epoch 64 | loss: 0.07353 |  0:00:04s\n",
      "epoch 65 | loss: 0.06559 |  0:00:04s\n",
      "epoch 66 | loss: 0.06057 |  0:00:04s\n",
      "epoch 67 | loss: 0.05252 |  0:00:04s\n",
      "epoch 68 | loss: 0.06154 |  0:00:04s\n",
      "epoch 69 | loss: 0.03822 |  0:00:04s\n",
      "epoch 70 | loss: 0.04336 |  0:00:04s\n",
      "epoch 71 | loss: 0.02182 |  0:00:05s\n",
      "epoch 72 | loss: 0.05509 |  0:00:05s\n",
      "epoch 73 | loss: 0.02621 |  0:00:05s\n",
      "epoch 74 | loss: 0.04182 |  0:00:05s\n",
      "epoch 75 | loss: 0.04955 |  0:00:05s\n",
      "epoch 76 | loss: 0.03712 |  0:00:05s\n",
      "epoch 77 | loss: 0.05211 |  0:00:05s\n",
      "epoch 78 | loss: 0.04188 |  0:00:05s\n",
      "epoch 79 | loss: 0.03492 |  0:00:05s\n",
      "epoch 80 | loss: 0.03805 |  0:00:05s\n",
      "epoch 81 | loss: 0.04535 |  0:00:05s\n",
      "epoch 82 | loss: 0.03496 |  0:00:05s\n",
      "epoch 83 | loss: 0.02761 |  0:00:05s\n",
      "epoch 84 | loss: 0.03115 |  0:00:05s\n",
      "epoch 85 | loss: 0.02479 |  0:00:06s\n",
      "epoch 86 | loss: 0.03375 |  0:00:06s\n",
      "epoch 87 | loss: 0.02942 |  0:00:06s\n",
      "epoch 88 | loss: 0.0344  |  0:00:06s\n",
      "epoch 89 | loss: 0.03241 |  0:00:06s\n",
      "epoch 90 | loss: 0.02931 |  0:00:06s\n",
      "epoch 91 | loss: 0.03251 |  0:00:06s\n",
      "epoch 92 | loss: 0.02629 |  0:00:06s\n",
      "epoch 93 | loss: 0.05954 |  0:00:06s\n",
      "epoch 94 | loss: 0.02502 |  0:00:06s\n",
      "epoch 95 | loss: 0.05175 |  0:00:06s\n",
      "epoch 96 | loss: 0.04013 |  0:00:06s\n",
      "epoch 97 | loss: 0.06775 |  0:00:06s\n",
      "epoch 98 | loss: 0.02197 |  0:00:06s\n",
      "epoch 99 | loss: 0.02824 |  0:00:06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:34:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18148\\2079566844.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m save_best_parameters(\n\u001b[1;32m--> 254\u001b[1;33m     \u001b[0msave_methods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_methods_rewrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"results/cc18_kappa_and_ece\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m )\n\u001b[0;32m    256\u001b[0m save_best_parameters(\n",
      "\u001b[1;32m~\\Documents\\GitHub\\df-dn-paper\\benchmarks\\tabular\\toolbox.py\u001b[0m in \u001b[0;36msave_best_parameters\u001b[1;34m(save_methods, save_methods_rewrite, path_save, best_parameters, non_json)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;31m#if not non_json:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_save\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_parameters_to_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m     \u001b[1;31m#else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m#    np.save(path_save + '.npy', best_parameters_to_save)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# a debuggability cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    374\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                 raise TypeError(f'keys must be str, int, float, bool or None, '\n\u001b[0m\u001b[0;32m    377\u001b[0m                                 f'not {key.__class__.__name__}')\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: keys must be str, int, float, bool or None, not int32"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Coauthors: Michael Ainsworth\n",
    "           Jayanta Dey\n",
    "           Haoyin Xu\n",
    "           Noga Mudrik\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import time\n",
    "from toolbox import *\n",
    "\n",
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "\n",
    "num_classes = 10\n",
    "reload_data = False  # indicator of whether to upload the data again\n",
    "\n",
    "path_save = \"metrics/cc18_all_parameters\"\n",
    "path_params = \"metrics/dict_parameters\"\n",
    "path_train_val_test_indices = \"metrics/dict_data_indices\"\n",
    "#with open(path_params + \".json\", \"r\") as json_file:\n",
    "#    dictionary_params = json.load(json_file)\n",
    "f = open('metrics/dict_parameters.json');\n",
    "dictionary_params = json.load(f)\n",
    "\n",
    "f2 = open(path_train_val_test_indices + \".json\")\n",
    "dict_data_indices = json.load(f2)\n",
    "\n",
    "f3 = open(\"metrics/varied_size_dict.json\") #varied size are final indices for each model. it is dict -> dataset -> index sampline -> list\n",
    "ss_inds_full = json.load(f3)\n",
    "\n",
    "models_to_scale = {\n",
    "    \"RF\": 0,\n",
    "    \"DN\": 1,\n",
    "    \"GBDT\": 0,\n",
    "} \n",
    "\n",
    "\n",
    "#f3 = open(ss_inds_path + \".json\")\n",
    "#dict_data_indices = json.load(f2)\n",
    "#with open(path_train_val_test_indices + \".json\", \"r\") as json_file:\n",
    "#    dict_data_indices = json.load(json_file)\n",
    "\n",
    "path = path_save  # \"metrics/cc18_all_parameters\"\n",
    "type_file = \".json\"\n",
    "dataset_indices_max = dictionary_params[\"dataset_indices_max\"]\n",
    "max_shape_to_run = dictionary_params[\"max_shape_to_run\"]\n",
    "file_type_to_load = \".json\"\n",
    "\n",
    "\"\"\"\n",
    "Number of repetitions for each CV fold at each sample size\n",
    "\"\"\"\n",
    "\n",
    "reps = 1\n",
    "shape_2_all_sample_sizes = dictionary_params[\"shape_2_all_sample_sizes\"]\n",
    "shape_2_evolution = dictionary_params[\"shape_2_evolution\"]\n",
    "n_splits = dictionary_params[\"shape_2_evolution\"]\n",
    "\n",
    "models_to_run = {\"RF\": 1, \"DN\": 1, \"GBDT\": 1}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Import CC18 data and pretuned hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "if reload_data or \"dataset_name\" not in locals():\n",
    "    X_data_list, y_data_list, dataset_name = load_cc18()\n",
    "\n",
    "\"\"\"\n",
    "Upload best parameters\n",
    "\"\"\"\n",
    "\n",
    "#if \"best_params_dict\" not in locals():\n",
    "f_best        = open(path+\".json\")\n",
    "best_params_dict = json.load(f_best)\n",
    "\n",
    "#train_indices = [i for i in range(dataset_indices_max)]\n",
    "\n",
    "\"\"\"\n",
    "Create empty arrays to index sample sizes, kappa values, and ece scores\n",
    "\"\"\"\n",
    "\n",
    "all_sample_sizes = np.zeros((dataset_indices_max, shape_2_all_sample_sizes))\n",
    "\n",
    "\"\"\"\n",
    "Empty arrays to index times into\n",
    "\"\"\"\n",
    "\n",
    "train_times = {\n",
    "    model_name: None for model_name in dictionary_params[\"classifiers_names\"]\n",
    "}\n",
    "test_times = {model_name: None for model_name in dictionary_params[\"classifiers_names\"]}\n",
    "\n",
    "train_test_times = {model_name: {} for model_name in best_params_dict.keys()}\n",
    "#train_test_times = {\n",
    "    #metric: {model_name: {} for model_name in best_params_dict.keys()}\n",
    "    #for metric in [\"cohen_kappa\", \"ece\"]\n",
    "#}\n",
    "\n",
    "\n",
    "evolution_dict = {\n",
    "    metric: {model_name: {} for model_name in best_params_dict.keys()}\n",
    "    for metric in [\"cohen_kappa\", \"ece\"]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "For each dataset, train and predict for every sample size\n",
    "Record outputs using Cohen's Kappa and ECE\n",
    "\"\"\"\n",
    "\n",
    "for dataset_index, dataset in enumerate(np.array(list(dict_data_indices.keys()),dtype = int)):\n",
    "    if len(dict_data_indices[str(dataset)]) > 0:\n",
    "        print(\"\\n\\nCurrent Dataset: \", dataset)\n",
    "    \n",
    "        X = X_data_list[dataset]\n",
    "        y = y_data_list[dataset]\n",
    "        \n",
    "            # If data set has over 10000 samples, resample to contain 10000\n",
    "        if X.shape[0] > max_shape_to_run:\n",
    "            X, y = sample_large_datasets(X, y, max_shape_to_run)\n",
    "            \n",
    "        train_indices = dict_data_indices[str(dataset_index)][\"train\"]\n",
    "        test_indices = dict_data_indices[str(dataset_index)][\"test\"]\n",
    "    \n",
    "    \n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "        training_sample_sizes = np.geomspace(\n",
    "            len(np.unique(y_train)) * 5, X_train.shape[0], num=8, dtype=int\n",
    "        )\n",
    "    \n",
    "        #ss_inds = random_sample_all[dataset]\n",
    "        #random_sample_new(\n",
    "        #    X_train, y_train, training_sample_sizes\n",
    "        #)\n",
    "    \n",
    "        # Iterate through each sample size per dataset\n",
    "        try:\n",
    "            ss_inds = ss_inds_full[dataset_index]\n",
    "        except:\n",
    "            ss_inds = ss_inds_full[str(dataset_index)]\n",
    "        for sample_size_index, real_sample_size in enumerate(training_sample_sizes):\n",
    "            real_sample_size  = int(real_sample_size)\n",
    "            cohen_ece_results_dict = {metric: {} for metric in [\"cohen_kappa\", \"ece\"]}\n",
    "            train_test_times_cur = {\n",
    "                model_name: np.zeros((reps)) for model_name in best_params_dict.keys()\n",
    "            }\n",
    "            \n",
    "            X_train_new = X_train[np.array(ss_inds[sample_size_index]).astype(int)]\n",
    "            y_train_new = y_train[np.array(ss_inds[sample_size_index]).astype(int)]\n",
    "    \n",
    "            # Repeat for number of repetitions, averaging results\n",
    "            for model_name, model_best_params in best_params_dict.items():\n",
    "                if models_to_run[model_name]:\n",
    "                    if models_to_scale[model_name ]:\n",
    "                        scaler = StandardScaler()\n",
    "                        scaler.fit(X_train_new)\n",
    "                        X_train_new = scaler.transform(X_train_new)\n",
    "        \n",
    "                    if dataset not in train_test_times[model_name].keys():\n",
    "                        train_test_times[model_name][dataset] = {}\n",
    "                        train_test_times[model_name][dataset] = {}\n",
    "    \n",
    "                    try:\n",
    "                        model = model_define(model_name, best_params_dict, dataset_index, sample_size_index)\n",
    "    \n",
    "                        start_time = time.perf_counter()\n",
    "                        model.fit(X_train_new, y_train_new)\n",
    "                        end_time = time.perf_counter()\n",
    "                        train_time = end_time - start_time\n",
    "    \n",
    "                        predictions = model.predict(X_test)\n",
    "                        predict_probas = model.predict_proba(X_test)\n",
    "    \n",
    "                        cohen_kappa = cohen_kappa_score(y_test, predictions)\n",
    "                        cohen_ece_results_dict[\"cohen_kappa\"][model_name] = cohen_kappa\n",
    "                        ece = get_ece(predict_probas, predictions, y_test)\n",
    "                        cohen_ece_results_dict[\"ece\"][model_name] = ece\n",
    "    \n",
    "                        train_test_times_cur[model_name] = train_time\n",
    "    \n",
    "                    except ValueError:\n",
    "                        print(model_name)\n",
    "    \n",
    "                    if dataset not in evolution_dict[\"cohen_kappa\"][model_name].keys():\n",
    "                        evolution_dict[\"cohen_kappa\"][model_name][dataset] = {}\n",
    "                        evolution_dict[\"ece\"][model_name][dataset] = {}\n",
    "                    if (\n",
    "                        real_sample_size\n",
    "                        not in evolution_dict[\"cohen_kappa\"][model_name][dataset].keys()\n",
    "                    ):\n",
    "                        evolution_dict[\"cohen_kappa\"][model_name][dataset][\n",
    "                            real_sample_size\n",
    "                        ] = []\n",
    "                        evolution_dict[\"ece\"][model_name][dataset][real_sample_size] = []\n",
    "                        train_test_times[model_name][dataset][real_sample_size] = []\n",
    "                        train_test_times[model_name][dataset][real_sample_size] = []\n",
    "    \n",
    "                    evolution_dict[\"cohen_kappa\"][model_name][dataset][\n",
    "                        real_sample_size\n",
    "                    ].append(cohen_ece_results_dict[\"cohen_kappa\"][model_name])\n",
    "                    \n",
    "                    evolution_dict[\"ece\"][model_name][dataset][real_sample_size].append(\n",
    "                        cohen_ece_results_dict[\"ece\"][model_name]\n",
    "                    )\n",
    "                    train_test_times[model_name][dataset][real_sample_size].append(\n",
    "                        train_test_times_cur[model_name]\n",
    "                    )\n",
    "    \n",
    "                    # Changing the results to tuple enabling easier saving to json and ectacting the fata after that.\n",
    "    \n",
    "                    train_test_times[model_name][dataset][real_sample_size] = tuple(\n",
    "                        train_test_times[model_name][dataset][real_sample_size]\n",
    "                    )\n",
    "    \n",
    "                    evolution_dict[\"cohen_kappa\"][model_name][dataset][\n",
    "                        real_sample_size\n",
    "                    ] = tuple(\n",
    "                        evolution_dict[\"cohen_kappa\"][model_name][dataset][real_sample_size]\n",
    "                    )\n",
    "    \n",
    "                    evolution_dict[\"ece\"][model_name][dataset][real_sample_size] = tuple(\n",
    "                        evolution_dict[\"ece\"][model_name][dataset][real_sample_size]\n",
    "                    )\n",
    "    \n",
    "        # Record sample sizes used\n",
    "        all_sample_sizes[dataset_index][:] = np.array(training_sample_sizes)\n",
    "\n",
    "new_dict = {}\n",
    "for key_met in evolution_dict.keys():\n",
    "    new_dict[key_met] = mod_dict(evolution_dict[key_met], tuple)\n",
    "#new_dict_times = mod_dict(train_test_times, tuple)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Save sample sizes and model results in json files\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "save_methods = {\"json\": 1}\n",
    "save_methods_rewrite = {\"json\": 1}\n",
    "save_best_parameters(\n",
    "    save_methods,\n",
    "    save_methods_rewrite,\n",
    "    \"metrics/cc18_sample_sizes.json\",\n",
    "    all_sample_sizes.tolist(),\n",
    ")\n",
    "\n",
    "save_best_parameters(\n",
    "    save_methods, save_methods_rewrite, \"results/cc18_kappa_and_ece\", new_dict\n",
    ")\n",
    "save_best_parameters(\n",
    "    save_methods, save_methods_rewrite, \"results/cc18_training_times\", train_test_times\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe79995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Dataset:  0\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 2.37175 |  0:00:00s\n",
      "epoch 1  | loss: 1.46474 |  0:00:00s\n",
      "epoch 2  | loss: 1.45166 |  0:00:00s\n",
      "epoch 3  | loss: 0.90884 |  0:00:00s\n",
      "epoch 4  | loss: 0.86081 |  0:00:00s\n",
      "epoch 5  | loss: 1.6017  |  0:00:00s\n",
      "epoch 6  | loss: 1.02068 |  0:00:00s\n",
      "epoch 7  | loss: 1.4734  |  0:00:00s\n",
      "epoch 8  | loss: 0.97165 |  0:00:01s\n",
      "epoch 9  | loss: 0.72757 |  0:00:01s\n",
      "epoch 10 | loss: 0.95151 |  0:00:01s\n",
      "epoch 11 | loss: 1.01741 |  0:00:01s\n",
      "epoch 12 | loss: 1.08218 |  0:00:01s\n",
      "epoch 13 | loss: 0.68702 |  0:00:01s\n",
      "epoch 14 | loss: 0.8023  |  0:00:01s\n",
      "epoch 15 | loss: 0.87479 |  0:00:01s\n",
      "epoch 16 | loss: 0.86579 |  0:00:01s\n",
      "epoch 17 | loss: 0.9773  |  0:00:01s\n",
      "epoch 18 | loss: 0.50087 |  0:00:02s\n",
      "epoch 19 | loss: 0.34849 |  0:00:02s\n",
      "epoch 20 | loss: 0.46568 |  0:00:02s\n",
      "epoch 21 | loss: 0.50124 |  0:00:02s\n",
      "epoch 22 | loss: 0.53685 |  0:00:02s\n",
      "epoch 23 | loss: 0.50596 |  0:00:02s\n",
      "epoch 24 | loss: 0.25919 |  0:00:02s\n",
      "epoch 25 | loss: 0.3108  |  0:00:02s\n",
      "epoch 26 | loss: 0.29055 |  0:00:02s\n",
      "epoch 27 | loss: 0.40127 |  0:00:02s\n",
      "epoch 28 | loss: 0.20617 |  0:00:02s\n",
      "epoch 29 | loss: 0.1275  |  0:00:02s\n",
      "epoch 30 | loss: 0.11145 |  0:00:02s\n",
      "epoch 31 | loss: 0.094   |  0:00:03s\n",
      "epoch 32 | loss: 0.09356 |  0:00:03s\n",
      "epoch 33 | loss: 0.10134 |  0:00:03s\n",
      "epoch 34 | loss: 0.09495 |  0:00:03s\n",
      "epoch 35 | loss: 0.10688 |  0:00:03s\n",
      "epoch 36 | loss: 0.09839 |  0:00:03s\n",
      "epoch 37 | loss: 0.07411 |  0:00:03s\n",
      "epoch 38 | loss: 0.07189 |  0:00:03s\n",
      "epoch 39 | loss: 0.08532 |  0:00:03s\n",
      "epoch 40 | loss: 0.07052 |  0:00:03s\n",
      "epoch 41 | loss: 0.06391 |  0:00:03s\n",
      "epoch 42 | loss: 0.07431 |  0:00:04s\n",
      "epoch 43 | loss: 0.08112 |  0:00:04s\n",
      "epoch 44 | loss: 0.06705 |  0:00:04s\n",
      "epoch 45 | loss: 0.07515 |  0:00:04s\n",
      "epoch 46 | loss: 0.06727 |  0:00:04s\n",
      "epoch 47 | loss: 0.06659 |  0:00:04s\n",
      "epoch 48 | loss: 0.06391 |  0:00:04s\n",
      "epoch 49 | loss: 0.06372 |  0:00:04s\n",
      "epoch 50 | loss: 0.06098 |  0:00:04s\n",
      "epoch 51 | loss: 0.05895 |  0:00:04s\n",
      "epoch 52 | loss: 0.05617 |  0:00:04s\n",
      "epoch 53 | loss: 0.0552  |  0:00:04s\n",
      "epoch 54 | loss: 0.057   |  0:00:04s\n",
      "epoch 55 | loss: 0.05361 |  0:00:05s\n",
      "epoch 56 | loss: 0.04948 |  0:00:05s\n",
      "epoch 57 | loss: 0.04502 |  0:00:05s\n",
      "epoch 58 | loss: 0.04381 |  0:00:05s\n",
      "epoch 59 | loss: 0.04212 |  0:00:05s\n",
      "epoch 60 | loss: 0.04612 |  0:00:05s\n",
      "epoch 61 | loss: 0.04158 |  0:00:05s\n",
      "epoch 62 | loss: 0.03907 |  0:00:05s\n",
      "epoch 63 | loss: 0.03803 |  0:00:05s\n",
      "epoch 64 | loss: 0.03643 |  0:00:05s\n",
      "epoch 65 | loss: 0.03567 |  0:00:05s\n",
      "epoch 66 | loss: 0.03412 |  0:00:05s\n",
      "epoch 67 | loss: 0.03275 |  0:00:05s\n",
      "epoch 68 | loss: 0.03127 |  0:00:05s\n",
      "epoch 69 | loss: 0.02966 |  0:00:05s\n",
      "epoch 70 | loss: 0.02853 |  0:00:06s\n",
      "epoch 71 | loss: 0.02758 |  0:00:06s\n",
      "epoch 72 | loss: 0.02625 |  0:00:06s\n",
      "epoch 73 | loss: 0.02539 |  0:00:06s\n",
      "epoch 74 | loss: 0.0243  |  0:00:06s\n",
      "epoch 75 | loss: 0.02327 |  0:00:06s\n",
      "epoch 76 | loss: 0.022   |  0:00:06s\n",
      "epoch 77 | loss: 0.02096 |  0:00:06s\n",
      "epoch 78 | loss: 0.01982 |  0:00:06s\n",
      "epoch 79 | loss: 0.01907 |  0:00:06s\n",
      "epoch 80 | loss: 0.01851 |  0:00:06s\n",
      "epoch 81 | loss: 0.01816 |  0:00:06s\n",
      "epoch 82 | loss: 0.01784 |  0:00:06s\n",
      "epoch 83 | loss: 0.01757 |  0:00:06s\n",
      "epoch 84 | loss: 0.01714 |  0:00:06s\n",
      "epoch 85 | loss: 0.01663 |  0:00:06s\n",
      "epoch 86 | loss: 0.0161  |  0:00:07s\n",
      "epoch 87 | loss: 0.01536 |  0:00:07s\n",
      "epoch 88 | loss: 0.01484 |  0:00:07s\n",
      "epoch 89 | loss: 0.01442 |  0:00:07s\n",
      "epoch 90 | loss: 0.01456 |  0:00:07s\n",
      "epoch 91 | loss: 0.01402 |  0:00:07s\n",
      "epoch 92 | loss: 0.01357 |  0:00:07s\n",
      "epoch 93 | loss: 0.01336 |  0:00:07s\n",
      "epoch 94 | loss: 0.01308 |  0:00:07s\n",
      "epoch 95 | loss: 0.01285 |  0:00:07s\n",
      "epoch 96 | loss: 0.01265 |  0:00:07s\n",
      "epoch 97 | loss: 0.01233 |  0:00:07s\n",
      "epoch 98 | loss: 0.01199 |  0:00:07s\n",
      "epoch 99 | loss: 0.0117  |  0:00:07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.09899 |  0:00:00s\n",
      "epoch 1  | loss: 1.27916 |  0:00:00s\n",
      "epoch 2  | loss: 1.10794 |  0:00:00s\n",
      "epoch 3  | loss: 0.76895 |  0:00:00s\n",
      "epoch 4  | loss: 0.77077 |  0:00:00s\n",
      "epoch 5  | loss: 0.91055 |  0:00:00s\n",
      "epoch 6  | loss: 0.60117 |  0:00:00s\n",
      "epoch 7  | loss: 0.81634 |  0:00:00s\n",
      "epoch 8  | loss: 0.4935  |  0:00:00s\n",
      "epoch 9  | loss: 0.59267 |  0:00:00s\n",
      "epoch 10 | loss: 0.33696 |  0:00:00s\n",
      "epoch 11 | loss: 0.60706 |  0:00:00s\n",
      "epoch 12 | loss: 0.30982 |  0:00:00s\n",
      "epoch 13 | loss: 0.3875  |  0:00:00s\n",
      "epoch 14 | loss: 0.3735  |  0:00:00s\n",
      "epoch 15 | loss: 0.39555 |  0:00:00s\n",
      "epoch 16 | loss: 0.27403 |  0:00:00s\n",
      "epoch 17 | loss: 0.39336 |  0:00:00s\n",
      "epoch 18 | loss: 0.4018  |  0:00:00s\n",
      "epoch 19 | loss: 0.26368 |  0:00:00s\n",
      "epoch 20 | loss: 0.3045  |  0:00:01s\n",
      "epoch 21 | loss: 0.38582 |  0:00:01s\n",
      "epoch 22 | loss: 0.22374 |  0:00:01s\n",
      "epoch 23 | loss: 0.30924 |  0:00:01s\n",
      "epoch 24 | loss: 0.45667 |  0:00:01s\n",
      "epoch 25 | loss: 0.46833 |  0:00:01s\n",
      "epoch 26 | loss: 0.26144 |  0:00:01s\n",
      "epoch 27 | loss: 0.20326 |  0:00:01s\n",
      "epoch 28 | loss: 0.21748 |  0:00:01s\n",
      "epoch 29 | loss: 0.09763 |  0:00:01s\n",
      "epoch 30 | loss: 0.08718 |  0:00:01s\n",
      "epoch 31 | loss: 0.07692 |  0:00:01s\n",
      "epoch 32 | loss: 0.03872 |  0:00:01s\n",
      "epoch 33 | loss: 0.02465 |  0:00:01s\n",
      "epoch 34 | loss: 0.01749 |  0:00:01s\n",
      "epoch 35 | loss: 0.02101 |  0:00:01s\n",
      "epoch 36 | loss: 0.01155 |  0:00:01s\n",
      "epoch 37 | loss: 0.01028 |  0:00:01s\n",
      "epoch 38 | loss: 0.00994 |  0:00:01s\n",
      "epoch 39 | loss: 0.00941 |  0:00:01s\n",
      "epoch 40 | loss: 0.0085  |  0:00:01s\n",
      "epoch 41 | loss: 0.00793 |  0:00:01s\n",
      "epoch 42 | loss: 0.0076  |  0:00:02s\n",
      "epoch 43 | loss: 0.00737 |  0:00:02s\n",
      "epoch 44 | loss: 0.00721 |  0:00:02s\n",
      "epoch 45 | loss: 0.00709 |  0:00:02s\n",
      "epoch 46 | loss: 0.00699 |  0:00:02s\n",
      "epoch 47 | loss: 0.00688 |  0:00:02s\n",
      "epoch 48 | loss: 0.00679 |  0:00:02s\n",
      "epoch 49 | loss: 0.00671 |  0:00:02s\n",
      "epoch 50 | loss: 0.00664 |  0:00:02s\n",
      "epoch 51 | loss: 0.00658 |  0:00:02s\n",
      "epoch 52 | loss: 0.00652 |  0:00:02s\n",
      "epoch 53 | loss: 0.00645 |  0:00:02s\n",
      "epoch 54 | loss: 0.00636 |  0:00:02s\n",
      "epoch 55 | loss: 0.00629 |  0:00:02s\n",
      "epoch 56 | loss: 0.00621 |  0:00:02s\n",
      "epoch 57 | loss: 0.00615 |  0:00:02s\n",
      "epoch 58 | loss: 0.0061  |  0:00:02s\n",
      "epoch 59 | loss: 0.00604 |  0:00:02s\n",
      "epoch 60 | loss: 0.00601 |  0:00:02s\n",
      "epoch 61 | loss: 0.00596 |  0:00:02s\n",
      "epoch 62 | loss: 0.0059  |  0:00:02s\n",
      "epoch 63 | loss: 0.00584 |  0:00:02s\n",
      "epoch 64 | loss: 0.00577 |  0:00:03s\n",
      "epoch 65 | loss: 0.0057  |  0:00:03s\n",
      "epoch 66 | loss: 0.00564 |  0:00:03s\n",
      "epoch 67 | loss: 0.00559 |  0:00:03s\n",
      "epoch 68 | loss: 0.00556 |  0:00:03s\n",
      "epoch 69 | loss: 0.00553 |  0:00:03s\n",
      "epoch 70 | loss: 0.00549 |  0:00:03s\n",
      "epoch 71 | loss: 0.00546 |  0:00:03s\n",
      "epoch 72 | loss: 0.00542 |  0:00:03s\n",
      "epoch 73 | loss: 0.00538 |  0:00:03s\n",
      "epoch 74 | loss: 0.00534 |  0:00:03s\n",
      "epoch 75 | loss: 0.00529 |  0:00:03s\n",
      "epoch 76 | loss: 0.00524 |  0:00:03s\n",
      "epoch 77 | loss: 0.00519 |  0:00:03s\n",
      "epoch 78 | loss: 0.00514 |  0:00:03s\n",
      "epoch 79 | loss: 0.00508 |  0:00:03s\n",
      "epoch 80 | loss: 0.00501 |  0:00:03s\n",
      "epoch 81 | loss: 0.00496 |  0:00:03s\n",
      "epoch 82 | loss: 0.00491 |  0:00:03s\n",
      "epoch 83 | loss: 0.00487 |  0:00:03s\n",
      "epoch 84 | loss: 0.00484 |  0:00:03s\n",
      "epoch 85 | loss: 0.00481 |  0:00:04s\n",
      "epoch 86 | loss: 0.00478 |  0:00:04s\n",
      "epoch 87 | loss: 0.00475 |  0:00:04s\n",
      "epoch 88 | loss: 0.00472 |  0:00:04s\n",
      "epoch 89 | loss: 0.0047  |  0:00:04s\n",
      "epoch 90 | loss: 0.00467 |  0:00:04s\n",
      "epoch 91 | loss: 0.00463 |  0:00:04s\n",
      "epoch 92 | loss: 0.00459 |  0:00:04s\n",
      "epoch 93 | loss: 0.00455 |  0:00:04s\n",
      "epoch 94 | loss: 0.00451 |  0:00:04s\n",
      "epoch 95 | loss: 0.00448 |  0:00:04s\n",
      "epoch 96 | loss: 0.00445 |  0:00:04s\n",
      "epoch 97 | loss: 0.00443 |  0:00:04s\n",
      "epoch 98 | loss: 0.0044  |  0:00:04s\n",
      "epoch 99 | loss: 0.00437 |  0:00:04s\n",
      "[13:45:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.96033 |  0:00:00s\n",
      "epoch 1  | loss: 1.50268 |  0:00:00s\n",
      "epoch 2  | loss: 1.08284 |  0:00:00s\n",
      "epoch 3  | loss: 1.29756 |  0:00:00s\n",
      "epoch 4  | loss: 1.19835 |  0:00:00s\n",
      "epoch 5  | loss: 1.35281 |  0:00:00s\n",
      "epoch 6  | loss: 1.29208 |  0:00:00s\n",
      "epoch 7  | loss: 1.551   |  0:00:00s\n",
      "epoch 8  | loss: 1.43929 |  0:00:00s\n",
      "epoch 9  | loss: 1.28527 |  0:00:00s\n",
      "epoch 10 | loss: 1.14703 |  0:00:00s\n",
      "epoch 11 | loss: 1.21114 |  0:00:00s\n",
      "epoch 12 | loss: 1.06401 |  0:00:00s\n",
      "epoch 13 | loss: 1.03252 |  0:00:00s\n",
      "epoch 14 | loss: 0.91138 |  0:00:00s\n",
      "epoch 15 | loss: 0.8598  |  0:00:00s\n",
      "epoch 16 | loss: 0.69804 |  0:00:00s\n",
      "epoch 17 | loss: 0.69556 |  0:00:00s\n",
      "epoch 18 | loss: 0.94139 |  0:00:00s\n",
      "epoch 19 | loss: 0.65413 |  0:00:00s\n",
      "epoch 20 | loss: 0.4347  |  0:00:01s\n",
      "epoch 21 | loss: 0.53832 |  0:00:01s\n",
      "epoch 22 | loss: 0.5373  |  0:00:01s\n",
      "epoch 23 | loss: 0.56489 |  0:00:01s\n",
      "epoch 24 | loss: 0.59678 |  0:00:01s\n",
      "epoch 25 | loss: 0.54236 |  0:00:01s\n",
      "epoch 26 | loss: 0.57783 |  0:00:01s\n",
      "epoch 27 | loss: 0.47262 |  0:00:01s\n",
      "epoch 28 | loss: 0.4886  |  0:00:01s\n",
      "epoch 29 | loss: 0.43265 |  0:00:01s\n",
      "epoch 30 | loss: 0.29141 |  0:00:01s\n",
      "epoch 31 | loss: 0.53672 |  0:00:01s\n",
      "epoch 32 | loss: 0.49293 |  0:00:01s\n",
      "epoch 33 | loss: 0.54947 |  0:00:01s\n",
      "epoch 34 | loss: 0.50948 |  0:00:01s\n",
      "epoch 35 | loss: 0.44007 |  0:00:01s\n",
      "epoch 36 | loss: 0.3997  |  0:00:01s\n",
      "epoch 37 | loss: 0.35946 |  0:00:01s\n",
      "epoch 38 | loss: 0.46973 |  0:00:02s\n",
      "epoch 39 | loss: 0.38426 |  0:00:02s\n",
      "epoch 40 | loss: 0.4685  |  0:00:02s\n",
      "epoch 41 | loss: 0.29876 |  0:00:02s\n",
      "epoch 42 | loss: 0.29647 |  0:00:02s\n",
      "epoch 43 | loss: 0.29803 |  0:00:02s\n",
      "epoch 44 | loss: 0.25405 |  0:00:02s\n",
      "epoch 45 | loss: 0.16442 |  0:00:02s\n",
      "epoch 46 | loss: 0.23587 |  0:00:02s\n",
      "epoch 47 | loss: 0.44128 |  0:00:02s\n",
      "epoch 48 | loss: 0.36009 |  0:00:02s\n",
      "epoch 49 | loss: 0.39642 |  0:00:02s\n",
      "epoch 50 | loss: 0.29116 |  0:00:02s\n",
      "epoch 51 | loss: 0.1732  |  0:00:02s\n",
      "epoch 52 | loss: 0.20782 |  0:00:02s\n",
      "epoch 53 | loss: 0.21248 |  0:00:02s\n",
      "epoch 54 | loss: 0.25514 |  0:00:02s\n",
      "epoch 55 | loss: 0.30954 |  0:00:02s\n",
      "epoch 56 | loss: 0.30436 |  0:00:03s\n",
      "epoch 57 | loss: 0.26692 |  0:00:03s\n",
      "epoch 58 | loss: 0.18814 |  0:00:03s\n",
      "epoch 59 | loss: 0.17631 |  0:00:03s\n",
      "epoch 60 | loss: 0.13156 |  0:00:03s\n",
      "epoch 61 | loss: 0.21046 |  0:00:03s\n",
      "epoch 62 | loss: 0.13762 |  0:00:03s\n",
      "epoch 63 | loss: 0.08275 |  0:00:03s\n",
      "epoch 64 | loss: 0.05189 |  0:00:03s\n",
      "epoch 65 | loss: 0.03326 |  0:00:03s\n",
      "epoch 66 | loss: 0.02226 |  0:00:03s\n",
      "epoch 67 | loss: 0.01541 |  0:00:03s\n",
      "epoch 68 | loss: 0.00919 |  0:00:03s\n",
      "epoch 69 | loss: 0.01938 |  0:00:03s\n",
      "epoch 70 | loss: 0.01329 |  0:00:03s\n",
      "epoch 71 | loss: 0.01059 |  0:00:03s\n",
      "epoch 72 | loss: 0.00947 |  0:00:03s\n",
      "epoch 73 | loss: 0.01061 |  0:00:04s\n",
      "epoch 74 | loss: 0.00168 |  0:00:04s\n",
      "epoch 75 | loss: 0.00179 |  0:00:04s\n",
      "epoch 76 | loss: 0.08495 |  0:00:04s\n",
      "epoch 77 | loss: 0.00671 |  0:00:04s\n",
      "epoch 78 | loss: 0.02866 |  0:00:04s\n",
      "epoch 79 | loss: 0.24584 |  0:00:04s\n",
      "epoch 80 | loss: 0.04689 |  0:00:04s\n",
      "epoch 81 | loss: 0.01855 |  0:00:04s\n",
      "epoch 82 | loss: 0.01097 |  0:00:04s\n",
      "epoch 83 | loss: 0.13679 |  0:00:04s\n",
      "epoch 84 | loss: 0.45581 |  0:00:04s\n",
      "epoch 85 | loss: 0.00575 |  0:00:04s\n",
      "epoch 86 | loss: 0.03475 |  0:00:04s\n",
      "epoch 87 | loss: 0.00631 |  0:00:04s\n",
      "epoch 88 | loss: 0.00254 |  0:00:04s\n",
      "epoch 89 | loss: 0.00203 |  0:00:04s\n",
      "epoch 90 | loss: 0.00212 |  0:00:04s\n",
      "epoch 91 | loss: 0.0014  |  0:00:05s\n",
      "epoch 92 | loss: 0.00122 |  0:00:05s\n",
      "epoch 93 | loss: 0.0009  |  0:00:05s\n",
      "epoch 94 | loss: 0.0008  |  0:00:05s\n",
      "epoch 95 | loss: 0.00053 |  0:00:05s\n",
      "epoch 96 | loss: 0.00108 |  0:00:05s\n",
      "epoch 97 | loss: 0.00098 |  0:00:05s\n",
      "epoch 98 | loss: 0.0008  |  0:00:05s\n",
      "epoch 99 | loss: 0.17376 |  0:00:05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 3.8337  |  0:00:00s\n",
      "epoch 1  | loss: 3.47912 |  0:00:00s\n",
      "epoch 2  | loss: 1.84334 |  0:00:00s\n",
      "epoch 3  | loss: 1.52741 |  0:00:00s\n",
      "epoch 4  | loss: 1.04863 |  0:00:00s\n",
      "epoch 5  | loss: 0.78537 |  0:00:00s\n",
      "epoch 6  | loss: 1.27258 |  0:00:00s\n",
      "epoch 7  | loss: 1.03999 |  0:00:00s\n",
      "epoch 8  | loss: 1.06168 |  0:00:00s\n",
      "epoch 9  | loss: 1.12504 |  0:00:00s\n",
      "epoch 10 | loss: 0.86983 |  0:00:00s\n",
      "epoch 11 | loss: 1.02653 |  0:00:01s\n",
      "epoch 12 | loss: 0.6861  |  0:00:01s\n",
      "epoch 13 | loss: 0.63442 |  0:00:01s\n",
      "epoch 14 | loss: 0.59223 |  0:00:01s\n",
      "epoch 15 | loss: 0.58344 |  0:00:01s\n",
      "epoch 16 | loss: 0.52946 |  0:00:01s\n",
      "epoch 17 | loss: 0.62911 |  0:00:01s\n",
      "epoch 18 | loss: 0.60015 |  0:00:01s\n",
      "epoch 19 | loss: 0.47164 |  0:00:01s\n",
      "epoch 20 | loss: 0.6215  |  0:00:01s\n",
      "epoch 21 | loss: 0.58903 |  0:00:01s\n",
      "epoch 22 | loss: 0.48527 |  0:00:02s\n",
      "epoch 23 | loss: 0.504   |  0:00:02s\n",
      "epoch 24 | loss: 0.38239 |  0:00:02s\n",
      "epoch 25 | loss: 0.46816 |  0:00:02s\n",
      "epoch 26 | loss: 0.30272 |  0:00:02s\n",
      "epoch 27 | loss: 0.48706 |  0:00:02s\n",
      "epoch 28 | loss: 0.34122 |  0:00:02s\n",
      "epoch 29 | loss: 0.42135 |  0:00:02s\n",
      "epoch 30 | loss: 0.32885 |  0:00:02s\n",
      "epoch 31 | loss: 0.42633 |  0:00:02s\n",
      "epoch 32 | loss: 0.33384 |  0:00:02s\n",
      "epoch 33 | loss: 0.2371  |  0:00:02s\n",
      "epoch 34 | loss: 0.28809 |  0:00:03s\n",
      "epoch 35 | loss: 0.43646 |  0:00:03s\n",
      "epoch 36 | loss: 0.33798 |  0:00:03s\n",
      "epoch 37 | loss: 0.6031  |  0:00:03s\n",
      "epoch 38 | loss: 0.66085 |  0:00:03s\n",
      "epoch 39 | loss: 0.63919 |  0:00:03s\n",
      "epoch 40 | loss: 0.32077 |  0:00:03s\n",
      "epoch 41 | loss: 0.58567 |  0:00:03s\n",
      "epoch 42 | loss: 0.61813 |  0:00:03s\n",
      "epoch 43 | loss: 0.60514 |  0:00:03s\n",
      "epoch 44 | loss: 0.4778  |  0:00:03s\n",
      "epoch 45 | loss: 0.58609 |  0:00:03s\n",
      "epoch 46 | loss: 0.30248 |  0:00:04s\n",
      "epoch 47 | loss: 0.39296 |  0:00:04s\n",
      "epoch 48 | loss: 0.40053 |  0:00:04s\n",
      "epoch 49 | loss: 0.34738 |  0:00:04s\n",
      "epoch 50 | loss: 0.36425 |  0:00:04s\n",
      "epoch 51 | loss: 0.25212 |  0:00:04s\n",
      "epoch 52 | loss: 0.25075 |  0:00:04s\n",
      "epoch 53 | loss: 0.20058 |  0:00:04s\n",
      "epoch 54 | loss: 0.27319 |  0:00:04s\n",
      "epoch 55 | loss: 0.21845 |  0:00:04s\n",
      "epoch 56 | loss: 0.31013 |  0:00:05s\n",
      "epoch 57 | loss: 0.27919 |  0:00:05s\n",
      "epoch 58 | loss: 0.27406 |  0:00:05s\n",
      "epoch 59 | loss: 0.28305 |  0:00:05s\n",
      "epoch 60 | loss: 0.5562  |  0:00:05s\n",
      "epoch 61 | loss: 0.41233 |  0:00:05s\n",
      "epoch 62 | loss: 0.36139 |  0:00:05s\n",
      "epoch 63 | loss: 0.24816 |  0:00:05s\n",
      "epoch 64 | loss: 0.43388 |  0:00:05s\n",
      "epoch 65 | loss: 0.31015 |  0:00:05s\n",
      "epoch 66 | loss: 0.25083 |  0:00:05s\n",
      "epoch 67 | loss: 0.51659 |  0:00:05s\n",
      "epoch 68 | loss: 0.2105  |  0:00:05s\n",
      "epoch 69 | loss: 0.28583 |  0:00:05s\n",
      "epoch 70 | loss: 0.29799 |  0:00:06s\n",
      "epoch 71 | loss: 0.34441 |  0:00:06s\n",
      "epoch 72 | loss: 0.28213 |  0:00:06s\n",
      "epoch 73 | loss: 0.17551 |  0:00:06s\n",
      "epoch 74 | loss: 0.22092 |  0:00:06s\n",
      "epoch 75 | loss: 0.20044 |  0:00:06s\n",
      "epoch 76 | loss: 0.26594 |  0:00:06s\n",
      "epoch 77 | loss: 0.13435 |  0:00:06s\n",
      "epoch 78 | loss: 0.15231 |  0:00:06s\n",
      "epoch 79 | loss: 0.39696 |  0:00:06s\n",
      "epoch 80 | loss: 0.25667 |  0:00:06s\n",
      "epoch 81 | loss: 0.15603 |  0:00:06s\n",
      "epoch 82 | loss: 0.20812 |  0:00:06s\n",
      "epoch 83 | loss: 0.17979 |  0:00:07s\n",
      "epoch 84 | loss: 0.23453 |  0:00:07s\n",
      "epoch 85 | loss: 0.13951 |  0:00:07s\n",
      "epoch 86 | loss: 0.11367 |  0:00:07s\n",
      "epoch 87 | loss: 0.10254 |  0:00:07s\n",
      "epoch 88 | loss: 0.1198  |  0:00:07s\n",
      "epoch 89 | loss: 0.1087  |  0:00:07s\n",
      "epoch 90 | loss: 0.08402 |  0:00:07s\n",
      "epoch 91 | loss: 0.10844 |  0:00:07s\n",
      "epoch 92 | loss: 0.04272 |  0:00:07s\n",
      "epoch 93 | loss: 0.03576 |  0:00:07s\n",
      "epoch 94 | loss: 0.18576 |  0:00:07s\n",
      "epoch 95 | loss: 0.11017 |  0:00:07s\n",
      "epoch 96 | loss: 0.05734 |  0:00:08s\n",
      "epoch 97 | loss: 0.03621 |  0:00:08s\n",
      "epoch 98 | loss: 0.11639 |  0:00:08s\n",
      "epoch 99 | loss: 0.07409 |  0:00:08s\n",
      "[13:45:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.70325 |  0:00:00s\n",
      "epoch 1  | loss: 1.25264 |  0:00:00s\n",
      "epoch 2  | loss: 1.08757 |  0:00:00s\n",
      "epoch 3  | loss: 0.82023 |  0:00:00s\n",
      "epoch 4  | loss: 0.88603 |  0:00:00s\n",
      "epoch 5  | loss: 0.97065 |  0:00:00s\n",
      "epoch 6  | loss: 0.78702 |  0:00:00s\n",
      "epoch 7  | loss: 0.94689 |  0:00:00s\n",
      "epoch 8  | loss: 0.74655 |  0:00:00s\n",
      "epoch 9  | loss: 0.8004  |  0:00:00s\n",
      "epoch 10 | loss: 0.82628 |  0:00:00s\n",
      "epoch 11 | loss: 0.69991 |  0:00:00s\n",
      "epoch 12 | loss: 0.78979 |  0:00:00s\n",
      "epoch 13 | loss: 0.80005 |  0:00:01s\n",
      "epoch 14 | loss: 0.75054 |  0:00:01s\n",
      "epoch 15 | loss: 0.71289 |  0:00:01s\n",
      "epoch 16 | loss: 0.6522  |  0:00:01s\n",
      "epoch 17 | loss: 0.6683  |  0:00:01s\n",
      "epoch 18 | loss: 0.62635 |  0:00:01s\n",
      "epoch 19 | loss: 0.6617  |  0:00:01s\n",
      "epoch 20 | loss: 0.67382 |  0:00:01s\n",
      "epoch 21 | loss: 0.57056 |  0:00:01s\n",
      "epoch 22 | loss: 0.57764 |  0:00:01s\n",
      "epoch 23 | loss: 0.59811 |  0:00:01s\n",
      "epoch 24 | loss: 0.48424 |  0:00:01s\n",
      "epoch 25 | loss: 0.62025 |  0:00:01s\n",
      "epoch 26 | loss: 0.47149 |  0:00:02s\n",
      "epoch 27 | loss: 0.54157 |  0:00:02s\n",
      "epoch 28 | loss: 0.46629 |  0:00:02s\n",
      "epoch 29 | loss: 0.43119 |  0:00:02s\n",
      "epoch 30 | loss: 0.45954 |  0:00:02s\n",
      "epoch 31 | loss: 0.49869 |  0:00:02s\n",
      "epoch 32 | loss: 0.48538 |  0:00:02s\n",
      "epoch 33 | loss: 0.44104 |  0:00:02s\n",
      "epoch 34 | loss: 0.48025 |  0:00:02s\n",
      "epoch 35 | loss: 0.42887 |  0:00:02s\n",
      "epoch 36 | loss: 0.40525 |  0:00:02s\n",
      "epoch 37 | loss: 0.40387 |  0:00:02s\n",
      "epoch 38 | loss: 0.33303 |  0:00:02s\n",
      "epoch 39 | loss: 0.41868 |  0:00:03s\n",
      "epoch 40 | loss: 0.36571 |  0:00:03s\n",
      "epoch 41 | loss: 0.33329 |  0:00:03s\n",
      "epoch 42 | loss: 0.39674 |  0:00:03s\n",
      "epoch 43 | loss: 0.43133 |  0:00:03s\n",
      "epoch 44 | loss: 0.40204 |  0:00:03s\n",
      "epoch 45 | loss: 0.3512  |  0:00:03s\n",
      "epoch 46 | loss: 0.30452 |  0:00:03s\n",
      "epoch 47 | loss: 0.32647 |  0:00:03s\n",
      "epoch 48 | loss: 0.28957 |  0:00:03s\n",
      "epoch 49 | loss: 0.26495 |  0:00:03s\n",
      "epoch 50 | loss: 0.26617 |  0:00:03s\n",
      "epoch 51 | loss: 0.33902 |  0:00:03s\n",
      "epoch 52 | loss: 0.37658 |  0:00:04s\n",
      "epoch 53 | loss: 0.29086 |  0:00:04s\n",
      "epoch 54 | loss: 0.25291 |  0:00:04s\n",
      "epoch 55 | loss: 0.22999 |  0:00:04s\n",
      "epoch 56 | loss: 0.22186 |  0:00:04s\n",
      "epoch 57 | loss: 0.20987 |  0:00:04s\n",
      "epoch 58 | loss: 0.21186 |  0:00:04s\n",
      "epoch 59 | loss: 0.20585 |  0:00:04s\n",
      "epoch 60 | loss: 0.13843 |  0:00:04s\n",
      "epoch 61 | loss: 0.14618 |  0:00:04s\n",
      "epoch 62 | loss: 0.14361 |  0:00:04s\n",
      "epoch 63 | loss: 0.17379 |  0:00:04s\n",
      "epoch 64 | loss: 0.14098 |  0:00:04s\n",
      "epoch 65 | loss: 0.10095 |  0:00:05s\n",
      "epoch 66 | loss: 0.10587 |  0:00:05s\n",
      "epoch 67 | loss: 0.11084 |  0:00:05s\n",
      "epoch 68 | loss: 0.09324 |  0:00:05s\n",
      "epoch 69 | loss: 0.13208 |  0:00:05s\n",
      "epoch 70 | loss: 0.07583 |  0:00:05s\n",
      "epoch 71 | loss: 0.09018 |  0:00:05s\n",
      "epoch 72 | loss: 0.09346 |  0:00:05s\n",
      "epoch 73 | loss: 0.08972 |  0:00:05s\n",
      "epoch 74 | loss: 0.04894 |  0:00:05s\n",
      "epoch 75 | loss: 0.02488 |  0:00:05s\n",
      "epoch 76 | loss: 0.04966 |  0:00:05s\n",
      "epoch 77 | loss: 0.03146 |  0:00:06s\n",
      "epoch 78 | loss: 0.02289 |  0:00:06s\n",
      "epoch 79 | loss: 0.02433 |  0:00:06s\n",
      "epoch 80 | loss: 0.13479 |  0:00:06s\n",
      "epoch 81 | loss: 0.22432 |  0:00:06s\n",
      "epoch 82 | loss: 0.12728 |  0:00:06s\n",
      "epoch 83 | loss: 0.09384 |  0:00:06s\n",
      "epoch 84 | loss: 0.05661 |  0:00:06s\n",
      "epoch 85 | loss: 0.03453 |  0:00:06s\n",
      "epoch 86 | loss: 0.02518 |  0:00:06s\n",
      "epoch 87 | loss: 0.05971 |  0:00:06s\n",
      "epoch 88 | loss: 0.02025 |  0:00:06s\n",
      "epoch 89 | loss: 0.05695 |  0:00:06s\n",
      "epoch 90 | loss: 0.06214 |  0:00:07s\n",
      "epoch 91 | loss: 0.1206  |  0:00:07s\n",
      "epoch 92 | loss: 0.0697  |  0:00:07s\n",
      "epoch 93 | loss: 0.0331  |  0:00:07s\n",
      "epoch 94 | loss: 0.04364 |  0:00:07s\n",
      "epoch 95 | loss: 0.05317 |  0:00:07s\n",
      "epoch 96 | loss: 0.03603 |  0:00:07s\n",
      "epoch 97 | loss: 0.16204 |  0:00:07s\n",
      "epoch 98 | loss: 0.07266 |  0:00:07s\n",
      "epoch 99 | loss: 0.034   |  0:00:07s\n",
      "[13:45:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.00373 |  0:00:00s\n",
      "epoch 1  | loss: 0.98285 |  0:00:00s\n",
      "epoch 2  | loss: 0.88016 |  0:00:00s\n",
      "epoch 3  | loss: 0.81731 |  0:00:00s\n",
      "epoch 4  | loss: 0.88326 |  0:00:00s\n",
      "epoch 5  | loss: 0.76181 |  0:00:00s\n",
      "epoch 6  | loss: 0.71542 |  0:00:00s\n",
      "epoch 7  | loss: 0.63783 |  0:00:00s\n",
      "epoch 8  | loss: 0.67842 |  0:00:00s\n",
      "epoch 9  | loss: 0.63587 |  0:00:00s\n",
      "epoch 10 | loss: 0.5769  |  0:00:00s\n",
      "epoch 11 | loss: 0.55214 |  0:00:00s\n",
      "epoch 12 | loss: 0.53201 |  0:00:00s\n",
      "epoch 13 | loss: 0.49119 |  0:00:00s\n",
      "epoch 14 | loss: 0.53824 |  0:00:00s\n",
      "epoch 15 | loss: 0.47421 |  0:00:00s\n",
      "epoch 16 | loss: 0.45083 |  0:00:00s\n",
      "epoch 17 | loss: 0.42523 |  0:00:00s\n",
      "epoch 18 | loss: 0.40808 |  0:00:00s\n",
      "epoch 19 | loss: 0.41714 |  0:00:00s\n",
      "epoch 20 | loss: 0.38787 |  0:00:00s\n",
      "epoch 21 | loss: 0.35781 |  0:00:00s\n",
      "epoch 22 | loss: 0.35476 |  0:00:00s\n",
      "epoch 23 | loss: 0.31335 |  0:00:00s\n",
      "epoch 24 | loss: 0.3266  |  0:00:01s\n",
      "epoch 25 | loss: 0.35812 |  0:00:01s\n",
      "epoch 26 | loss: 0.29771 |  0:00:01s\n",
      "epoch 27 | loss: 0.27513 |  0:00:01s\n",
      "epoch 28 | loss: 0.28571 |  0:00:01s\n",
      "epoch 29 | loss: 0.28727 |  0:00:01s\n",
      "epoch 30 | loss: 0.30435 |  0:00:01s\n",
      "epoch 31 | loss: 0.26526 |  0:00:01s\n",
      "epoch 32 | loss: 0.32064 |  0:00:01s\n",
      "epoch 33 | loss: 0.28113 |  0:00:01s\n",
      "epoch 34 | loss: 0.3118  |  0:00:01s\n",
      "epoch 35 | loss: 0.2611  |  0:00:01s\n",
      "epoch 36 | loss: 0.2934  |  0:00:01s\n",
      "epoch 37 | loss: 0.30867 |  0:00:01s\n",
      "epoch 38 | loss: 0.26607 |  0:00:01s\n",
      "epoch 39 | loss: 0.21863 |  0:00:01s\n",
      "epoch 40 | loss: 0.21161 |  0:00:01s\n",
      "epoch 41 | loss: 0.20169 |  0:00:01s\n",
      "epoch 42 | loss: 0.18354 |  0:00:01s\n",
      "epoch 43 | loss: 0.27362 |  0:00:01s\n",
      "epoch 44 | loss: 0.26361 |  0:00:01s\n",
      "epoch 45 | loss: 0.20793 |  0:00:01s\n",
      "epoch 46 | loss: 0.20501 |  0:00:01s\n",
      "epoch 47 | loss: 0.24869 |  0:00:01s\n",
      "epoch 48 | loss: 0.23246 |  0:00:02s\n",
      "epoch 49 | loss: 0.21155 |  0:00:02s\n",
      "epoch 50 | loss: 0.19697 |  0:00:02s\n",
      "epoch 51 | loss: 0.18676 |  0:00:02s\n",
      "epoch 52 | loss: 0.17386 |  0:00:02s\n",
      "epoch 53 | loss: 0.19325 |  0:00:02s\n",
      "epoch 54 | loss: 0.1932  |  0:00:02s\n",
      "epoch 55 | loss: 0.19011 |  0:00:02s\n",
      "epoch 56 | loss: 0.17859 |  0:00:02s\n",
      "epoch 57 | loss: 0.15157 |  0:00:02s\n",
      "epoch 58 | loss: 0.14381 |  0:00:02s\n",
      "epoch 59 | loss: 0.21299 |  0:00:02s\n",
      "epoch 60 | loss: 0.15798 |  0:00:02s\n",
      "epoch 61 | loss: 0.13081 |  0:00:02s\n",
      "epoch 62 | loss: 0.14769 |  0:00:02s\n",
      "epoch 63 | loss: 0.12839 |  0:00:02s\n",
      "epoch 64 | loss: 0.11098 |  0:00:02s\n",
      "epoch 65 | loss: 0.12194 |  0:00:02s\n",
      "epoch 66 | loss: 0.12229 |  0:00:02s\n",
      "epoch 67 | loss: 0.10637 |  0:00:02s\n",
      "epoch 68 | loss: 0.13367 |  0:00:02s\n",
      "epoch 69 | loss: 0.10429 |  0:00:02s\n",
      "epoch 70 | loss: 0.0976  |  0:00:02s\n",
      "epoch 71 | loss: 0.10414 |  0:00:03s\n",
      "epoch 72 | loss: 0.08589 |  0:00:03s\n",
      "epoch 73 | loss: 0.12134 |  0:00:03s\n",
      "epoch 74 | loss: 0.08928 |  0:00:03s\n",
      "epoch 75 | loss: 0.09656 |  0:00:03s\n",
      "epoch 76 | loss: 0.10213 |  0:00:03s\n",
      "epoch 77 | loss: 0.10623 |  0:00:03s\n",
      "epoch 78 | loss: 0.0899  |  0:00:03s\n",
      "epoch 79 | loss: 0.07929 |  0:00:03s\n",
      "epoch 80 | loss: 0.06858 |  0:00:03s\n",
      "epoch 81 | loss: 0.12954 |  0:00:03s\n",
      "epoch 82 | loss: 0.08121 |  0:00:03s\n",
      "epoch 83 | loss: 0.0716  |  0:00:03s\n",
      "epoch 84 | loss: 0.09053 |  0:00:03s\n",
      "epoch 85 | loss: 0.13881 |  0:00:03s\n",
      "epoch 86 | loss: 0.1078  |  0:00:03s\n",
      "epoch 87 | loss: 0.09376 |  0:00:03s\n",
      "epoch 88 | loss: 0.05844 |  0:00:03s\n",
      "epoch 89 | loss: 0.0547  |  0:00:03s\n",
      "epoch 90 | loss: 0.04085 |  0:00:03s\n",
      "epoch 91 | loss: 0.04702 |  0:00:03s\n",
      "epoch 92 | loss: 0.05549 |  0:00:03s\n",
      "epoch 93 | loss: 0.04634 |  0:00:03s\n",
      "epoch 94 | loss: 0.04252 |  0:00:03s\n",
      "epoch 95 | loss: 0.09165 |  0:00:04s\n",
      "epoch 96 | loss: 0.06457 |  0:00:04s\n",
      "epoch 97 | loss: 0.05266 |  0:00:04s\n",
      "epoch 98 | loss: 0.07812 |  0:00:04s\n",
      "epoch 99 | loss: 0.05545 |  0:00:04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.99937 |  0:00:00s\n",
      "epoch 1  | loss: 0.90945 |  0:00:00s\n",
      "epoch 2  | loss: 0.75641 |  0:00:00s\n",
      "epoch 3  | loss: 0.7016  |  0:00:00s\n",
      "epoch 4  | loss: 0.69314 |  0:00:00s\n",
      "epoch 5  | loss: 0.68586 |  0:00:00s\n",
      "epoch 6  | loss: 0.67249 |  0:00:00s\n",
      "epoch 7  | loss: 0.67692 |  0:00:00s\n",
      "epoch 8  | loss: 0.6602  |  0:00:00s\n",
      "epoch 9  | loss: 0.61932 |  0:00:00s\n",
      "epoch 10 | loss: 0.62992 |  0:00:00s\n",
      "epoch 11 | loss: 0.59722 |  0:00:00s\n",
      "epoch 12 | loss: 0.54761 |  0:00:01s\n",
      "epoch 13 | loss: 0.5966  |  0:00:01s\n",
      "epoch 14 | loss: 0.48866 |  0:00:01s\n",
      "epoch 15 | loss: 0.54141 |  0:00:01s\n",
      "epoch 16 | loss: 0.4657  |  0:00:01s\n",
      "epoch 17 | loss: 0.51833 |  0:00:01s\n",
      "epoch 18 | loss: 0.47709 |  0:00:01s\n",
      "epoch 19 | loss: 0.46908 |  0:00:01s\n",
      "epoch 20 | loss: 0.44703 |  0:00:01s\n",
      "epoch 21 | loss: 0.38792 |  0:00:01s\n",
      "epoch 22 | loss: 0.40099 |  0:00:01s\n",
      "epoch 23 | loss: 0.43001 |  0:00:01s\n",
      "epoch 24 | loss: 0.45829 |  0:00:02s\n",
      "epoch 25 | loss: 0.37737 |  0:00:02s\n",
      "epoch 26 | loss: 0.3979  |  0:00:02s\n",
      "epoch 27 | loss: 0.41701 |  0:00:02s\n",
      "epoch 28 | loss: 0.38192 |  0:00:02s\n",
      "epoch 29 | loss: 0.42275 |  0:00:02s\n",
      "epoch 30 | loss: 0.36126 |  0:00:02s\n",
      "epoch 31 | loss: 0.37274 |  0:00:02s\n",
      "epoch 32 | loss: 0.34491 |  0:00:02s\n",
      "epoch 33 | loss: 0.36772 |  0:00:02s\n",
      "epoch 34 | loss: 0.32195 |  0:00:02s\n",
      "epoch 35 | loss: 0.28593 |  0:00:02s\n",
      "epoch 36 | loss: 0.34066 |  0:00:03s\n",
      "epoch 37 | loss: 0.37229 |  0:00:03s\n",
      "epoch 38 | loss: 0.35607 |  0:00:03s\n",
      "epoch 39 | loss: 0.34383 |  0:00:03s\n",
      "epoch 40 | loss: 0.30052 |  0:00:03s\n",
      "epoch 41 | loss: 0.29164 |  0:00:03s\n",
      "epoch 42 | loss: 0.26335 |  0:00:03s\n",
      "epoch 43 | loss: 0.25213 |  0:00:03s\n",
      "epoch 44 | loss: 0.25285 |  0:00:03s\n",
      "epoch 45 | loss: 0.25084 |  0:00:03s\n",
      "epoch 46 | loss: 0.23286 |  0:00:03s\n",
      "epoch 47 | loss: 0.23821 |  0:00:03s\n",
      "epoch 48 | loss: 0.26187 |  0:00:03s\n",
      "epoch 49 | loss: 0.25328 |  0:00:04s\n",
      "epoch 50 | loss: 0.25939 |  0:00:04s\n",
      "epoch 51 | loss: 0.23459 |  0:00:04s\n",
      "epoch 52 | loss: 0.22992 |  0:00:04s\n",
      "epoch 53 | loss: 0.24324 |  0:00:04s\n",
      "epoch 54 | loss: 0.21486 |  0:00:04s\n",
      "epoch 55 | loss: 0.24377 |  0:00:04s\n",
      "epoch 56 | loss: 0.20516 |  0:00:04s\n",
      "epoch 57 | loss: 0.18664 |  0:00:04s\n",
      "epoch 58 | loss: 0.18035 |  0:00:04s\n",
      "epoch 59 | loss: 0.20044 |  0:00:04s\n",
      "epoch 60 | loss: 0.17312 |  0:00:04s\n",
      "epoch 61 | loss: 0.1801  |  0:00:05s\n",
      "epoch 62 | loss: 0.21113 |  0:00:05s\n",
      "epoch 63 | loss: 0.17774 |  0:00:05s\n",
      "epoch 64 | loss: 0.21456 |  0:00:05s\n",
      "epoch 65 | loss: 0.17818 |  0:00:05s\n",
      "epoch 66 | loss: 0.16641 |  0:00:05s\n",
      "epoch 67 | loss: 0.18327 |  0:00:05s\n",
      "epoch 68 | loss: 0.1796  |  0:00:05s\n",
      "epoch 69 | loss: 0.13913 |  0:00:05s\n",
      "epoch 70 | loss: 0.12614 |  0:00:05s\n",
      "epoch 71 | loss: 0.19151 |  0:00:05s\n",
      "epoch 72 | loss: 0.15209 |  0:00:05s\n",
      "epoch 73 | loss: 0.13324 |  0:00:05s\n",
      "epoch 74 | loss: 0.15952 |  0:00:06s\n",
      "epoch 75 | loss: 0.12355 |  0:00:06s\n",
      "epoch 76 | loss: 0.22188 |  0:00:06s\n",
      "epoch 77 | loss: 0.17966 |  0:00:06s\n",
      "epoch 78 | loss: 0.14091 |  0:00:06s\n",
      "epoch 79 | loss: 0.16405 |  0:00:06s\n",
      "epoch 80 | loss: 0.11309 |  0:00:06s\n",
      "epoch 81 | loss: 0.13066 |  0:00:06s\n",
      "epoch 82 | loss: 0.15443 |  0:00:06s\n",
      "epoch 83 | loss: 0.10874 |  0:00:06s\n",
      "epoch 84 | loss: 0.1579  |  0:00:06s\n",
      "epoch 85 | loss: 0.10172 |  0:00:06s\n",
      "epoch 86 | loss: 0.10675 |  0:00:06s\n",
      "epoch 87 | loss: 0.08721 |  0:00:07s\n",
      "epoch 88 | loss: 0.10923 |  0:00:07s\n",
      "epoch 89 | loss: 0.09739 |  0:00:07s\n",
      "epoch 90 | loss: 0.12094 |  0:00:07s\n",
      "epoch 91 | loss: 0.11332 |  0:00:07s\n",
      "epoch 92 | loss: 0.09753 |  0:00:07s\n",
      "epoch 93 | loss: 0.09457 |  0:00:07s\n",
      "epoch 94 | loss: 0.10069 |  0:00:07s\n",
      "epoch 95 | loss: 0.10252 |  0:00:07s\n",
      "epoch 96 | loss: 0.06926 |  0:00:07s\n",
      "epoch 97 | loss: 0.06131 |  0:00:07s\n",
      "epoch 98 | loss: 0.0567  |  0:00:07s\n",
      "epoch 99 | loss: 0.05781 |  0:00:08s\n",
      "[13:46:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.84024 |  0:00:00s\n",
      "epoch 1  | loss: 0.69435 |  0:00:00s\n",
      "epoch 2  | loss: 0.68492 |  0:00:00s\n",
      "epoch 3  | loss: 0.65718 |  0:00:00s\n",
      "epoch 4  | loss: 0.63858 |  0:00:00s\n",
      "epoch 5  | loss: 0.63143 |  0:00:00s\n",
      "epoch 6  | loss: 0.61764 |  0:00:00s\n",
      "epoch 7  | loss: 0.59993 |  0:00:00s\n",
      "epoch 8  | loss: 0.58261 |  0:00:00s\n",
      "epoch 9  | loss: 0.55777 |  0:00:00s\n",
      "epoch 10 | loss: 0.54863 |  0:00:00s\n",
      "epoch 11 | loss: 0.53419 |  0:00:00s\n",
      "epoch 12 | loss: 0.51024 |  0:00:00s\n",
      "epoch 13 | loss: 0.50221 |  0:00:00s\n",
      "epoch 14 | loss: 0.45094 |  0:00:01s\n",
      "epoch 15 | loss: 0.43925 |  0:00:01s\n",
      "epoch 16 | loss: 0.41772 |  0:00:01s\n",
      "epoch 17 | loss: 0.41244 |  0:00:01s\n",
      "epoch 18 | loss: 0.3997  |  0:00:01s\n",
      "epoch 19 | loss: 0.33797 |  0:00:01s\n",
      "epoch 20 | loss: 0.34652 |  0:00:01s\n",
      "epoch 21 | loss: 0.30844 |  0:00:01s\n",
      "epoch 22 | loss: 0.30946 |  0:00:01s\n",
      "epoch 23 | loss: 0.26453 |  0:00:01s\n",
      "epoch 24 | loss: 0.28897 |  0:00:01s\n",
      "epoch 25 | loss: 0.24441 |  0:00:01s\n",
      "epoch 26 | loss: 0.2232  |  0:00:02s\n",
      "epoch 27 | loss: 0.20623 |  0:00:02s\n",
      "epoch 28 | loss: 0.22637 |  0:00:02s\n",
      "epoch 29 | loss: 0.1843  |  0:00:02s\n",
      "epoch 30 | loss: 0.15407 |  0:00:02s\n",
      "epoch 31 | loss: 0.14387 |  0:00:02s\n",
      "epoch 32 | loss: 0.15725 |  0:00:02s\n",
      "epoch 33 | loss: 0.11736 |  0:00:02s\n",
      "epoch 34 | loss: 0.12029 |  0:00:02s\n",
      "epoch 35 | loss: 0.10073 |  0:00:02s\n",
      "epoch 36 | loss: 0.08433 |  0:00:03s\n",
      "epoch 37 | loss: 0.09603 |  0:00:03s\n",
      "epoch 38 | loss: 0.14445 |  0:00:03s\n",
      "epoch 39 | loss: 0.08547 |  0:00:03s\n",
      "epoch 40 | loss: 0.10502 |  0:00:03s\n",
      "epoch 41 | loss: 0.10941 |  0:00:03s\n",
      "epoch 42 | loss: 0.1258  |  0:00:03s\n",
      "epoch 43 | loss: 0.07634 |  0:00:03s\n",
      "epoch 44 | loss: 0.05577 |  0:00:03s\n",
      "epoch 45 | loss: 0.09633 |  0:00:03s\n",
      "epoch 46 | loss: 0.11396 |  0:00:03s\n",
      "epoch 47 | loss: 0.11996 |  0:00:03s\n",
      "epoch 48 | loss: 0.07432 |  0:00:04s\n",
      "epoch 49 | loss: 0.08152 |  0:00:04s\n",
      "epoch 50 | loss: 0.08127 |  0:00:04s\n",
      "epoch 51 | loss: 0.10342 |  0:00:04s\n",
      "epoch 52 | loss: 0.07854 |  0:00:04s\n",
      "epoch 53 | loss: 0.10836 |  0:00:04s\n",
      "epoch 54 | loss: 0.08167 |  0:00:04s\n",
      "epoch 55 | loss: 0.08475 |  0:00:04s\n",
      "epoch 56 | loss: 0.08823 |  0:00:04s\n",
      "epoch 57 | loss: 0.08745 |  0:00:04s\n",
      "epoch 58 | loss: 0.07696 |  0:00:04s\n",
      "epoch 59 | loss: 0.05648 |  0:00:04s\n",
      "epoch 60 | loss: 0.06112 |  0:00:05s\n",
      "epoch 61 | loss: 0.06521 |  0:00:05s\n",
      "epoch 62 | loss: 0.07373 |  0:00:05s\n",
      "epoch 63 | loss: 0.06355 |  0:00:05s\n",
      "epoch 64 | loss: 0.07353 |  0:00:05s\n",
      "epoch 65 | loss: 0.06559 |  0:00:05s\n",
      "epoch 66 | loss: 0.06057 |  0:00:05s\n",
      "epoch 67 | loss: 0.05252 |  0:00:05s\n",
      "epoch 68 | loss: 0.06154 |  0:00:05s\n",
      "epoch 69 | loss: 0.03822 |  0:00:05s\n",
      "epoch 70 | loss: 0.04336 |  0:00:05s\n",
      "epoch 71 | loss: 0.02182 |  0:00:05s\n",
      "epoch 72 | loss: 0.05509 |  0:00:05s\n",
      "epoch 73 | loss: 0.02621 |  0:00:05s\n",
      "epoch 74 | loss: 0.04182 |  0:00:05s\n",
      "epoch 75 | loss: 0.04955 |  0:00:06s\n",
      "epoch 76 | loss: 0.03712 |  0:00:06s\n",
      "epoch 77 | loss: 0.05211 |  0:00:06s\n",
      "epoch 78 | loss: 0.04188 |  0:00:06s\n",
      "epoch 79 | loss: 0.03492 |  0:00:06s\n",
      "epoch 80 | loss: 0.03805 |  0:00:06s\n",
      "epoch 81 | loss: 0.04535 |  0:00:06s\n",
      "epoch 82 | loss: 0.03496 |  0:00:06s\n",
      "epoch 83 | loss: 0.02761 |  0:00:06s\n",
      "epoch 84 | loss: 0.03115 |  0:00:06s\n",
      "epoch 85 | loss: 0.02479 |  0:00:06s\n",
      "epoch 86 | loss: 0.03375 |  0:00:06s\n",
      "epoch 87 | loss: 0.02942 |  0:00:06s\n",
      "epoch 88 | loss: 0.0344  |  0:00:07s\n",
      "epoch 89 | loss: 0.03241 |  0:00:07s\n",
      "epoch 90 | loss: 0.02931 |  0:00:07s\n",
      "epoch 91 | loss: 0.03251 |  0:00:07s\n",
      "epoch 92 | loss: 0.02629 |  0:00:07s\n",
      "epoch 93 | loss: 0.05954 |  0:00:07s\n",
      "epoch 94 | loss: 0.02502 |  0:00:07s\n",
      "epoch 95 | loss: 0.05175 |  0:00:07s\n",
      "epoch 96 | loss: 0.04013 |  0:00:07s\n",
      "epoch 97 | loss: 0.06775 |  0:00:07s\n",
      "epoch 98 | loss: 0.02197 |  0:00:08s\n",
      "epoch 99 | loss: 0.02824 |  0:00:08s\n",
      "[13:46:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noga mudrik\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18148\\2528097834.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnot_pyplot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcc18_kappa_ece_times\u001b[0m \u001b[1;31m# main_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcc18_hyperparameter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc18_hyperparameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcc18_hyperparameter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\df-dn-paper\\benchmarks\\tabular\\cc18_kappa_ece_times.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m save_best_parameters(\n\u001b[1;32m--> 254\u001b[1;33m     \u001b[0msave_methods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_methods_rewrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"results/cc18_kappa_and_ece\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m )\n\u001b[0;32m    256\u001b[0m save_best_parameters(\n",
      "\u001b[1;32m~\\Documents\\GitHub\\df-dn-paper\\benchmarks\\tabular\\toolbox.py\u001b[0m in \u001b[0;36msave_best_parameters\u001b[1;34m(save_methods, save_methods_rewrite, path_save, best_parameters, non_json)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;31m#if not non_json:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_save\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_parameters_to_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m     \u001b[1;31m#else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m#    np.save(path_save + '.npy', best_parameters_to_save)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# a debuggability cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    374\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                 raise TypeError(f'keys must be str, int, float, bool or None, '\n\u001b[0m\u001b[0;32m    377\u001b[0m                                 f'not {key.__class__.__name__}')\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: keys must be str, int, float, bool or None, not int32"
     ]
    }
   ],
   "source": [
    "not_pyplot = True\n",
    "import cc18_kappa_ece_times # main_functions\n",
    "from importlib import reload  \n",
    "cc18_hyperparameter = reload(cc18_hyperparameter)\n",
    "from cc18_hyperparameter import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "942449ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cohen_kappa': {'RF': {0: {10: (0.2739516180506798,),\n",
       "    17: (0.3171154958544894,),\n",
       "    30: (0.7526648369183635,),\n",
       "    53: (0.6433434926399066,),\n",
       "    93: (0.7921596214107566,),\n",
       "    163: (0.855444908448442,),\n",
       "    285: (0.9198769309659637,),\n",
       "    500: (0.9679507723863855,)}},\n",
       "  'DN': {0: {10: (0.17047517686095282,),\n",
       "    17: (-0.024778219639033372,),\n",
       "    30: (0.16414839580788276,),\n",
       "    53: (0.009587855383181187,),\n",
       "    93: (0.43564745141660677,),\n",
       "    163: (0.05793302313388826,),\n",
       "    285: (0.48323751695626904,),\n",
       "    500: (0.17066281284897844,)}},\n",
       "  'GBDT': {0: {10: (0.0,),\n",
       "    17: (0.0,),\n",
       "    30: (0.0,),\n",
       "    53: (0.09066860657363918,),\n",
       "    93: (0.0,),\n",
       "    163: (0.0008060724121716767,),\n",
       "    285: (0.13096735747864674,),\n",
       "    500: (0.13096735747864674,)}}},\n",
       " 'ece': {'RF': {0: {10: (0.12503333333333333,),\n",
       "    17: (0.12144000000000002,),\n",
       "    30: (0.18308000000000002,),\n",
       "    53: (0.07200000000000002,),\n",
       "    93: (0.08779579365079368,),\n",
       "    163: (0.04039999999999999,),\n",
       "    285: (0.08666301004999788,),\n",
       "    500: (0.03174666666666674,)}},\n",
       "  'DN': {0: {10: (0.3740966494083405,),\n",
       "    17: (0.5317989809513092,),\n",
       "    30: (0.4089048011302948,),\n",
       "    53: (0.4161111195087433,),\n",
       "    93: (0.21855832862854005,),\n",
       "    163: (0.3240749988555908,),\n",
       "    285: (0.20598112773895264,),\n",
       "    500: (0.36084294390678406,)}},\n",
       "  'GBDT': {0: {10: (0.22308183813095095,),\n",
       "    17: (0.2955447418689728,),\n",
       "    30: (0.20399961662292485,),\n",
       "    53: (0.35215734791755676,),\n",
       "    93: (0.2507591965198517,),\n",
       "    163: (0.38668943548202517,),\n",
       "    285: (0.3669556653499603,),\n",
       "    500: (0.37433671188354495,)}}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649643b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
